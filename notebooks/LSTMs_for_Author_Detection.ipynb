{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMs for Author Detection",
      "provenance": [],
      "collapsed_sections": [
        "0GAWvK9P0HPD",
        "fw0kgJiZ0Lbz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeAMwrKQTYgX",
        "colab_type": "text"
      },
      "source": [
        "# LSTM for Author Detection\n",
        "\n",
        "> This notebook contains implementation of LSTM model for author detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GObQ2DPTjPQ",
        "colab_type": "text"
      },
      "source": [
        "### Imports and Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRHHCfmAkhj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DOsOTA3TRef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os, glob, re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import seaborn as sn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Bidirectional, BatchNormalization, SpatialDropout1D, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxN9AqunTvu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "isOnColab = True\n",
        "\n",
        "if isOnColab == True:\n",
        "  DIR_PREFIX = './drive/My Drive/MSCS/S4/NLP/project/'\n",
        "else:\n",
        "  DIR_PREFIX = './'\n",
        "\n",
        "DATASET_DIR = DIR_PREFIX + 'dataset/'\n",
        "MODEL_SAVE_DIR = DIR_PREFIX + 'models/'\n",
        "\n",
        "TRAIN_FILE = DATASET_DIR + 'train.csv'\n",
        "VAL_FILE = DATASET_DIR + 'val.csv'\n",
        "TEST_FILE = DATASET_DIR + 'test.csv'\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 100\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92bsQVa7UrY3",
        "colab_type": "text"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEsHSxfBUpMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(file_name):\n",
        "  dF = pd.read_csv(file_name, delimiter=',')\n",
        "  return (dF['Text'], dF['Label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEUEVTM8U8jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_train_X, raw_train_Y = load_dataset(TRAIN_FILE)\n",
        "raw_val_X, raw_val_Y     = load_dataset(VAL_FILE)\n",
        "raw_test_X, raw_test_Y   = load_dataset(TEST_FILE)\n",
        "\n",
        "print('train_X: {0}\\ttrain_Y: {1}'.format(len(raw_train_X), len(raw_train_Y)))\n",
        "print('val_X: {0}\\tval_Y: {1}'.format(len(raw_val_X), len(raw_val_Y)))\n",
        "print('test_X: {0}\\ttest_Y: {1}'.format(len(raw_test_X), len(raw_test_Y)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_zEiCVvVdwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot = raw_train_Y.value_counts().sort_values(ascending=False).plot(kind='bar', y='# of tweets', title='# of tweets per politician') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kV2F9ciXYQp",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VaRE1BMWMTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(data):\n",
        "    processed = []\n",
        "    for each in data:\n",
        "        target = each\n",
        "\n",
        "        # tokenize words\n",
        "        target = word_tokenize(target)\n",
        "\n",
        "        # truncate sentence for MAX_SENTENCE_LENGTH\n",
        "        target = target[:MAX_SENTENCE_LENGTH]\n",
        "\n",
        "        # finally append to processed\n",
        "        processed.append(target)\n",
        "\n",
        "    return processed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbzcLEKCkWLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_train_X = preprocessing(raw_train_X)\n",
        "processed_val_X = preprocessing(raw_val_X)\n",
        "processed_test_X = preprocessing(raw_test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghEV_IGkkcJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create one-hot labels\n",
        "labels_dict = {}\n",
        "\n",
        "for index, x in enumerate(list(raw_train_Y.unique())):\n",
        "  labels_dict[x] = index\n",
        "\n",
        "# replace string labels with corresponding numeric value from labels_dict\n",
        "train_Y = raw_train_Y.apply(lambda x: labels_dict[x])\n",
        "val_Y = raw_val_Y.apply(lambda x: labels_dict[x])\n",
        "test_Y = raw_test_Y.apply(lambda x: labels_dict[x])\n",
        "\n",
        "# convert to one-hot encoding\n",
        "train_Y = to_categorical(train_Y)\n",
        "val_Y = to_categorical(val_Y)\n",
        "test_Y = to_categorical(test_Y)\n",
        "\n",
        "# print\n",
        "train_Y, val_Y, test_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJDFGHdXlseq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize texts\n",
        "tokenizer = Tokenizer(oov_token='[UNK]')\n",
        "tokenizer.fit_on_texts(processed_train_X)\n",
        "\n",
        "print('word counts:', tokenizer.word_counts)\n",
        "print('document_count:', tokenizer.document_count)\n",
        "print('vocab_size:', len(tokenizer.word_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9iRUdVzmzcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = tokenizer.texts_to_sequences(processed_train_X)\n",
        "val_X = tokenizer.texts_to_sequences(processed_val_X)\n",
        "test_X = tokenizer.texts_to_sequences(processed_test_X)\n",
        "\n",
        "print(train_X[:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B74Lw2ZitKsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pad sequences\n",
        "\n",
        "train_X = pad_sequences(train_X,  maxlen=MAX_SENTENCE_LENGTH, padding='post')\n",
        "val_X = pad_sequences(val_X,      maxlen=MAX_SENTENCE_LENGTH, padding='post')\n",
        "test_X = pad_sequences(test_X,    maxlen=MAX_SENTENCE_LENGTH, padding='post')\n",
        "\n",
        "print(train_X[:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lACTd4d1vf2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4_Zc8G-OSiV",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBQQufKUOWmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GLOVE_DIR = DATASET_DIR + 'glove.twitter.27B.100d.txt'\n",
        "GLOVE_DIR = DATASET_DIR + 'glove.6B.100d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUxuGwCZOqKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load embeddings of GLoVE\n",
        "embeddings_dict = {}\n",
        "\n",
        "with open(GLOVE_DIR, 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      embeddings_dict[word] = vector\n",
        "\n",
        "print('embeddings_dict:', len(embeddings_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPViiLEaOxKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# padding\n",
        "embeddings_dict['[UNK]'] = np.zeros((EMBEDDING_DIM,), dtype='float32')\n",
        "\n",
        "# load embeddings for train dataset\n",
        "for X in processed_train_X:\n",
        "  for sentence in X:\n",
        "    words = sentence.split(' ')\n",
        "    for word in words:\n",
        "      try:\n",
        "        embdng = embeddings_dict[word]\n",
        "      except KeyError:\n",
        "        embdng = np.zeros((EMBEDDING_DIM,), dtype='float32')\n",
        "    \n",
        "      # add embedding to DICT\n",
        "      embeddings_dict[word] = embdng\n",
        "\n",
        "print('embeddings_dict:', len(embeddings_dict))\n",
        "\n",
        "# load embeddings for validation dataset\n",
        "for X in processed_val_X:\n",
        "  for sentence in X:\n",
        "    words = sentence.split(' ')\n",
        "    for word in words:\n",
        "      try:\n",
        "        embdng = embeddings_dict[word]\n",
        "      except KeyError:\n",
        "        embdng = np.zeros((EMBEDDING_DIM,), dtype='float32')\n",
        "    \n",
        "      # add embedding to DICT\n",
        "      embeddings_dict[word] = embdng\n",
        "\n",
        "print('embeddings_dict:', len(embeddings_dict))\n",
        "\n",
        "# load embeddings for test dataset\n",
        "for X in processed_test_X:\n",
        "  for sentence in X:\n",
        "    words = sentence.split(' ')\n",
        "    for word in words:\n",
        "      try:\n",
        "        embdng = embeddings_dict[word]\n",
        "      except KeyError:\n",
        "        embdng = np.zeros((EMBEDDING_DIM,), dtype='float32')\n",
        "    \n",
        "      # add embedding to DICT\n",
        "      embeddings_dict[word] = embdng\n",
        "\n",
        "print('embeddings_dict:', len(embeddings_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0gdmwmgPVj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create matrix of embeddings\n",
        "EMBEDDING_MATRIX = np.zeros((len(tokenizer.word_index) + 1, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  try:\n",
        "    vector = embeddings_dict[word]\n",
        "  except KeyError:\n",
        "    vector = np.zeros((EMBEDDING_DIM,), dtype='float32')\n",
        "  # append vector\n",
        "  EMBEDDING_MATRIX[i] = vector \n",
        "\n",
        "print('EMBEDDING_MATRIX', EMBEDDING_MATRIX.shape, EMBEDDING_MATRIX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHjJubxUwu9I",
        "colab_type": "text"
      },
      "source": [
        "### Simple LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_bNf0iRxyVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup callbacks\n",
        "\n",
        "MODEL_SAVE_AT = MODEL_SAVE_DIR + 'lstm_plain/'\n",
        "if not os.path.exists(MODEL_SAVE_AT):\n",
        "  os.makedirs(MODEL_SAVE_AT)\n",
        "\n",
        "\n",
        "filepath = MODEL_SAVE_AT + 'model' + '.hdf5'\n",
        "logfilepath = MODEL_SAVE_AT + 'logs_' + 'model' + '.csv'\n",
        "\n",
        "reduce_lr_rate=0.2\n",
        "logCallback = CSVLogger(logfilepath, separator=',', append=False)\n",
        "earlyStopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_weights_only=True, verbose=1,\n",
        "                             save_best_only=True, mode='auto')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=reduce_lr_rate, patience=3,\n",
        "                              cooldown=0, min_lr=0.0000000001, verbose=0)\n",
        "\n",
        "callbacks_list = [logCallback, earlyStopping, reduce_lr, checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSG2UCdEwo-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Code\n",
        "\n",
        "inp_layer = Input(shape=(MAX_SENTENCE_LENGTH,))\n",
        "\n",
        "layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=EMBEDDING_DIM, \n",
        "                  input_length=MAX_SENTENCE_LENGTH, weights=[EMBEDDING_MATRIX], trainable=True)(inp_layer)\n",
        "\n",
        "layer = SpatialDropout1D(0.3)(layer)\n",
        "layer = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(layer)\n",
        "layer = BatchNormalization()(layer)\n",
        "layer = Dropout(0.5)(layer)\n",
        "layer = Dense(len(labels_dict), activation='sigmoid')(layer)\n",
        "\n",
        "output_layer = layer\n",
        "\n",
        "model = Model(inputs=inp_layer, outputs=output_layer)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GAWvK9P0HPD",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyHCt7Pez_CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlQfZNpCzRR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = model.fit(train_X, train_Y, validation_data=(val_X, val_Y), epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "               verbose=1, shuffle=True, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw0kgJiZ0Lbz",
        "colab_type": "text"
      },
      "source": [
        "#### Inference and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX6Ub4O30KdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_X, batch_size=BATCH_SIZE, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAoJHQp82tJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_list = labels_dict.keys()\n",
        "\n",
        "test_Y_max = np.argmax(test_Y, axis=-1)\n",
        "predictions_max = np.argmax(predictions, axis=-1)\n",
        "\n",
        "####################################### CONFUSION MATRIX\n",
        "\n",
        "cm = confusion_matrix(test_Y_max, predictions_max)\n",
        "print('Confusion Matrix:\\n', cm)\n",
        "\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "cm = pd.DataFrame(cm, labels_list, labels_list)\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(cm, annot=True, annot_kws={\"size\": 11}, fmt=\".2f\") # font size\n",
        "plt.show()\n",
        "\n",
        "report_labels = [str(x) for x in labels_dict.values()]\n",
        "#########################################################\n",
        "\n",
        "################################### CLASSIFICATION REPORT\n",
        "print(\"\\n\\nClassification Report\\n\", classification_report(test_Y_max, predictions_max, \n",
        "                                                       labels=list(labels_dict.values()), target_names=report_labels))\n",
        "#########################################################\n",
        "\n",
        "\n",
        "################################# MODEL TRAINING PROGRESS\n",
        "logsdF = pd.read_csv(logfilepath, delimiter=',')\n",
        "ax = plt.gca()\n",
        "\n",
        "logsdF.plot(kind='line', x='epoch', y='accuracy', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='loss', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='val_accuracy', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='val_loss', ax=ax)\n",
        "plt.legend()\n",
        "plt.title('Model Training Progress')\n",
        "plt.show()\n",
        "#########################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAhxtZdfwV5B",
        "colab_type": "text"
      },
      "source": [
        "### BiDirectional LSTM with 1D CNN and Max Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REJ4tqQuwYPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup callbacks\n",
        "\n",
        "MODEL_SAVE_AT = MODEL_SAVE_DIR + 'lstm_bidirectional/'\n",
        "if not os.path.exists(MODEL_SAVE_AT):\n",
        "  os.makedirs(MODEL_SAVE_AT)\n",
        "\n",
        "\n",
        "filepath = MODEL_SAVE_AT + 'model' + '.hdf5'\n",
        "logfilepath = MODEL_SAVE_AT + 'logs_' + 'model' + '.csv'\n",
        "\n",
        "reduce_lr_rate=0.2\n",
        "logCallback = CSVLogger(logfilepath, separator=',', append=False)\n",
        "earlyStopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_weights_only=True, verbose=1,\n",
        "                             save_best_only=True, mode='auto')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=reduce_lr_rate, patience=3,\n",
        "                              cooldown=0, min_lr=0.0000000001, verbose=0)\n",
        "\n",
        "callbacks_list = [logCallback, earlyStopping, reduce_lr, checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_pMQSVawm3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Code\n",
        "\n",
        "inp_layer = Input(shape=(MAX_SENTENCE_LENGTH,))\n",
        "\n",
        "layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=EMBEDDING_DIM, \n",
        "                  input_length=MAX_SENTENCE_LENGTH, weights=[EMBEDDING_MATRIX], \n",
        "\t\t\t\t\t\t\t\t\tembeddings_regularizer=regularizers.l2(0.00), trainable=True)(inp_layer)\n",
        "\n",
        "layer = SpatialDropout1D(0.3)(layer)\n",
        "\n",
        "layer = Bidirectional(LSTM(128, name='blstm_1',\n",
        "\tactivation='tanh',\n",
        "\trecurrent_activation='hard_sigmoid',\n",
        "\trecurrent_dropout=0.0,\n",
        "\tdropout=0.5, \n",
        "\tkernel_initializer='glorot_uniform',\n",
        "\treturn_sequences=True))(layer)\n",
        " \n",
        "layer = Conv1D(filters=64, kernel_size=2, padding='valid', kernel_initializer = 'glorot_uniform', name='conv2d_1')(layer)\n",
        "layer = GlobalMaxPooling1D()(layer)\n",
        "# layer = BatchNormalization()(layer)\n",
        "# layer = Dropout(0.5)(layer)\n",
        "layer = Dense(len(labels_dict), activation='sigmoid')(layer)\n",
        "\n",
        "output_layer = layer\n",
        "\n",
        "model = Model(inputs=inp_layer, outputs=output_layer)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdVdwbDwwbTx",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp7F8aGOwcZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJbcGxMqwyfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = model.fit(train_X, train_Y, validation_data=(val_X, val_Y), epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "               verbose=1, shuffle=True, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVoVC5rrwc4m",
        "colab_type": "text"
      },
      "source": [
        "#### Inference and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_W05yLBwerz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_X, batch_size=BATCH_SIZE, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4FoMSrlw471",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_list = labels_dict.keys()\n",
        "\n",
        "test_Y_max = np.argmax(test_Y, axis=-1)\n",
        "predictions_max = np.argmax(predictions, axis=-1)\n",
        "\n",
        "####################################### CONFUSION MATRIX\n",
        "\n",
        "cm = confusion_matrix(test_Y_max, predictions_max)\n",
        "print('Confusion Matrix:\\n', cm)\n",
        "\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "cm = pd.DataFrame(cm, labels_list, labels_list)\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(cm, annot=True, annot_kws={\"size\": 11}, fmt=\".2f\") # font size\n",
        "plt.show()\n",
        "\n",
        "report_labels = [str(x) for x in labels_dict.values()]\n",
        "#########################################################\n",
        "\n",
        "################################### CLASSIFICATION REPORT\n",
        "print(\"\\n\\nClassification Report\\n\", classification_report(test_Y_max, predictions_max, \n",
        "                                                       labels=list(labels_dict.values()), target_names=report_labels))\n",
        "#########################################################\n",
        "\n",
        "\n",
        "################################# MODEL TRAINING PROGRESS\n",
        "logsdF = pd.read_csv(logfilepath, delimiter=',')\n",
        "ax = plt.gca()\n",
        "\n",
        "logsdF.plot(kind='line', x='epoch', y='accuracy', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='loss', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='val_accuracy', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='val_loss', ax=ax)\n",
        "plt.legend()\n",
        "plt.title('Model Training Progress')\n",
        "plt.show()\n",
        "#########################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJJ7V9jey8x6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}