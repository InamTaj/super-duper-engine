{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN for Author Detection",
      "provenance": [],
      "collapsed_sections": [
        "0GAWvK9P0HPD",
        "fw0kgJiZ0Lbz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeAMwrKQTYgX",
        "colab_type": "text"
      },
      "source": [
        "# CNN for Author Detection\n",
        "\n",
        "> This notebook contains implementation of CNN model for author detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GObQ2DPTjPQ",
        "colab_type": "text"
      },
      "source": [
        "### Imports and Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRHHCfmAkhj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DOsOTA3TRef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os, glob, re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import seaborn as sn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical, get_source_inputs\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, InputSpec, Input, Dense, Embedding, Flatten, BatchNormalization, Activation, Conv1D, Add, MaxPooling1D, ThresholdedReLU, Convolution1D, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxN9AqunTvu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "isOnColab = True\n",
        "\n",
        "if isOnColab == True:\n",
        "  DIR_PREFIX = './drive/My Drive/MSCS/S4/NLP/project/'\n",
        "else:\n",
        "  DIR_PREFIX = './'\n",
        "\n",
        "DATASET_DIR = DIR_PREFIX + 'dataset/'\n",
        "MODEL_SAVE_DIR = DIR_PREFIX + 'models/'\n",
        "\n",
        "TRAIN_FILE = DATASET_DIR + 'train.csv'\n",
        "VAL_FILE = DATASET_DIR + 'val.csv'\n",
        "TEST_FILE = DATASET_DIR + 'test.csv'\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 100\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92bsQVa7UrY3",
        "colab_type": "text"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEsHSxfBUpMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(file_name):\n",
        "  dF = pd.read_csv(file_name, delimiter=',')\n",
        "  return (dF['Text'], dF['Label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEUEVTM8U8jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_train_X, raw_train_Y = load_dataset(TRAIN_FILE)\n",
        "raw_val_X, raw_val_Y     = load_dataset(VAL_FILE)\n",
        "raw_test_X, raw_test_Y   = load_dataset(TEST_FILE)\n",
        "\n",
        "print('train_X: {0}\\ttrain_Y: {1}'.format(len(raw_train_X), len(raw_train_Y)))\n",
        "print('val_X: {0}\\tval_Y: {1}'.format(len(raw_val_X), len(raw_val_Y)))\n",
        "print('test_X: {0}\\ttest_Y: {1}'.format(len(raw_test_X), len(raw_test_Y)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_zEiCVvVdwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot = raw_train_Y.value_counts().sort_values(ascending=False).plot(kind='bar', y='# of tweets', title='# of tweets per politician') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kV2F9ciXYQp",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VaRE1BMWMTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(data):\n",
        "    processed = []\n",
        "    for each in data:\n",
        "        target = each\n",
        "\n",
        "        # # tokenize words\n",
        "        # target = word_tokenize(target)\n",
        "\n",
        "        # truncate sentence for MAX_SENTENCE_LENGTH\n",
        "        target = target[:MAX_SENTENCE_LENGTH]\n",
        "\n",
        "        # finally append to processed\n",
        "        processed.append(target)\n",
        "\n",
        "    return processed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbzcLEKCkWLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_train_X = preprocessing(raw_train_X)\n",
        "processed_val_X = preprocessing(raw_val_X)\n",
        "processed_test_X = preprocessing(raw_test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghEV_IGkkcJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create one-hot labels\n",
        "labels_dict = {}\n",
        "\n",
        "for index, x in enumerate(list(raw_train_Y.unique())):\n",
        "  labels_dict[x] = index\n",
        "\n",
        "# replace string labels with corresponding numeric value from labels_dict\n",
        "train_Y = raw_train_Y.apply(lambda x: labels_dict[x])\n",
        "val_Y = raw_val_Y.apply(lambda x: labels_dict[x])\n",
        "test_Y = raw_test_Y.apply(lambda x: labels_dict[x])\n",
        "\n",
        "# convert to one-hot encoding\n",
        "train_Y = to_categorical(train_Y)\n",
        "val_Y = to_categorical(val_Y)\n",
        "test_Y = to_categorical(test_Y)\n",
        "\n",
        "# print\n",
        "train_Y, val_Y, test_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJDFGHdXlseq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize texts\n",
        "tokenizer = Tokenizer(oov_token='[UNK]', char_level=True)\n",
        "tokenizer.fit_on_texts(processed_train_X)\n",
        "\n",
        "print('char counts:', tokenizer.word_counts)\n",
        "print('document_count:', tokenizer.document_count)\n",
        "print('vocab_size:', len(tokenizer.word_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9iRUdVzmzcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = tokenizer.texts_to_sequences(processed_train_X)\n",
        "val_X = tokenizer.texts_to_sequences(processed_val_X)\n",
        "test_X = tokenizer.texts_to_sequences(processed_test_X)\n",
        "\n",
        "print(len(train_X[:1][0]), train_X[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B74Lw2ZitKsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pad sequences\n",
        "\n",
        "train_X = pad_sequences(train_X,  maxlen=MAX_SENTENCE_LENGTH, padding='post')\n",
        "val_X = pad_sequences(val_X,      maxlen=MAX_SENTENCE_LENGTH, padding='post')\n",
        "test_X = pad_sequences(test_X,    maxlen=MAX_SENTENCE_LENGTH, padding='post')\n",
        "\n",
        "print(train_X[:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lACTd4d1vf2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHjJubxUwu9I",
        "colab_type": "text"
      },
      "source": [
        "### VDCNN - Very Deep CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD7rauf6PlCm",
        "colab_type": "text"
      },
      "source": [
        "#### Model\n",
        "\n",
        "Reference: https://github.com/zonetrooper32/VDCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bbNKzrBQSmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KMaxPooling(Layer):\n",
        "    \"\"\"\n",
        "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
        "    TensorFlow backend.\n",
        "    \"\"\"\n",
        "    def __init__(self, k=1, sorted=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.input_spec = InputSpec(ndim=3)\n",
        "        self.k = k\n",
        "        self.sorted = sorted\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.k, input_shape[2])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # swap last two dimensions since top_k will be applied along the last dimension\n",
        "        shifted_inputs = tf.transpose(inputs, [0, 2, 1])\n",
        "        \n",
        "        # extract top_k, returns two tensors [values, indices]\n",
        "        top_k = tf.nn.top_k(shifted_inputs, k=self.k, sorted=self.sorted)[0]\n",
        "        \n",
        "        # return flattened output\n",
        "        return tf.transpose(top_k, [0,2,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJJ7V9jey8x6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(inputs, filters, kernel_size=3, use_bias=False, shortcut=False):\n",
        "    conv1 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding='same')(inputs)\n",
        "    bn1 = BatchNormalization()(conv1)\n",
        "    relu = Activation('relu')(bn1)\n",
        "    conv2 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding='same')(relu)\n",
        "    out = BatchNormalization()(conv2)\n",
        "    if shortcut:\n",
        "        out = Add()([out, inputs])\n",
        "    return Activation('relu')(out)\n",
        "\n",
        "def conv_block(inputs, filters, kernel_size=3, use_bias=False, shortcut=False, \n",
        "               pool_type='max', sorted=True, stage=1):\n",
        "    conv1 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding='same')(inputs)\n",
        "    bn1 = BatchNormalization()(conv1)\n",
        "    relu1 = Activation('relu')(bn1)\n",
        "\n",
        "    conv2 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding='same')(relu1)\n",
        "    out = BatchNormalization()(conv2)\n",
        "\n",
        "    if shortcut:\n",
        "        residual = Conv1D(filters=filters, kernel_size=1, strides=2, name='shortcut_conv1d_%d' % stage)(inputs)\n",
        "        residual = BatchNormalization(name='shortcut_batch_normalization_%d' % stage)(residual)\n",
        "        out = downsample(out, pool_type=pool_type, sorted=sorted, stage=stage)\n",
        "        out = Add()([out, residual])\n",
        "        out = Activation('relu')(out)\n",
        "    else:\n",
        "        out = Activation('relu')(out)\n",
        "        out = downsample(out, pool_type=pool_type, sorted=sorted, stage=stage)\n",
        "    if pool_type is not None:\n",
        "        out = Conv1D(filters=2*filters, kernel_size=1, strides=1, padding='same', name='1_1_conv_%d' % stage)(out)\n",
        "        out = BatchNormalization(name='1_1_batch_normalization_%d' % stage)(out)\n",
        "    return out\n",
        "\n",
        "def downsample(inputs, pool_type='max', sorted=True, stage=1):\n",
        "    if pool_type == 'max':\n",
        "        out = MaxPooling1D(pool_size=3, strides=2, padding='same', name='pool_%d' % stage)(inputs)\n",
        "    elif pool_type == 'k_max':\n",
        "        k = int(inputs._keras_shape[1]/2)\n",
        "        out = KMaxPooling(k=k, sorted=sorted, name='pool_%d' % stage)(inputs)\n",
        "    elif pool_type == 'conv':\n",
        "        out = Conv1D(filters=inputs._keras_shape[-1], kernel_size=3, strides=2, padding='same', name='pool_%d' % stage)(inputs)\n",
        "        out = BatchNormalization()(out)\n",
        "    elif pool_type is None:\n",
        "        out = inputs\n",
        "    else:\n",
        "        raise ValueError('unsupported pooling type!')\n",
        "    return out\n",
        "\n",
        "def VDCNN(num_classes, depth=9, sequence_length=MAX_SENTENCE_LENGTH, embedding_dim=EMBEDDING_DIM, \n",
        "          shortcut=False, pool_type='max', sorted=True, use_bias=False, input_tensor=None):\n",
        "    if depth == 9:\n",
        "        num_conv_blocks = (1, 1, 1, 1)\n",
        "    elif depth == 17:\n",
        "        num_conv_blocks = (2, 2, 2, 2)\n",
        "    elif depth == 29:\n",
        "        num_conv_blocks = (5, 5, 2, 2)\n",
        "    elif depth == 49:\n",
        "        num_conv_blocks = (8, 8, 5, 3)\n",
        "    else:\n",
        "        raise ValueError('unsupported depth for VDCNN.')\n",
        "\n",
        "    inputs = Input(shape=(sequence_length, ), name='inputs')\n",
        "    embedded_chars = Embedding(input_dim=sequence_length, output_dim=embedding_dim)(inputs)\n",
        "    out = Conv1D(filters=64, kernel_size=3, strides=1, padding='same', name='temp_conv')(embedded_chars)\n",
        "\n",
        "    # Convolutional Block 64\n",
        "    for _ in range(num_conv_blocks[0] - 1):\n",
        "        out = identity_block(out, filters=64, kernel_size=3, use_bias=use_bias, shortcut=shortcut)\n",
        "    out = conv_block(out, filters=64, kernel_size=3, use_bias=use_bias, shortcut=shortcut, \n",
        "                     pool_type=pool_type, sorted=sorted, stage=1)\n",
        "\n",
        "    # Convolutional Block 128\n",
        "    for _ in range(num_conv_blocks[1] - 1):\n",
        "        out = identity_block(out, filters=128, kernel_size=3, use_bias=use_bias, shortcut=shortcut)\n",
        "    out = conv_block(out, filters=128, kernel_size=3, use_bias=use_bias, shortcut=shortcut, \n",
        "                     pool_type=pool_type, sorted=sorted, stage=2)\n",
        "\n",
        "    # Convolutional Block 256\n",
        "    for _ in range(num_conv_blocks[2] - 1):\n",
        "        out = identity_block(out, filters=256, kernel_size=3, use_bias=use_bias, shortcut=shortcut)\n",
        "    out = conv_block(out, filters=256, kernel_size=3, use_bias=use_bias, shortcut=shortcut, \n",
        "                     pool_type=pool_type, sorted=sorted, stage=3)\n",
        "\n",
        "    # Convolutional Block 512\n",
        "    for _ in range(num_conv_blocks[3] - 1):\n",
        "        out = identity_block(out, filters=512, kernel_size=3, use_bias=use_bias, shortcut=shortcut)\n",
        "    out = conv_block(out, filters=512, kernel_size=3, use_bias=use_bias, shortcut=False, \n",
        "                     pool_type=None, stage=4)\n",
        "\n",
        "    # k-max pooling with k = 8\n",
        "    out = KMaxPooling(k=8, sorted=True)(out)\n",
        "    out = Flatten()(out)\n",
        "\n",
        "    # Dense Layers\n",
        "    out = Dense(2048, activation='relu')(out)\n",
        "    out = Dense(2048, activation='relu')(out)\n",
        "    out = Dense(num_classes, activation='softmax')(out)\n",
        "\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = inputs\n",
        "\n",
        "    # Create model.\n",
        "    model = Model(inputs=inputs, outputs=out, name='VDCNN')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01IklWv-v6M-",
        "colab_type": "text"
      },
      "source": [
        "#### Depth = 9 (conv layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0lu37C3Uj7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup callbacks\n",
        "\n",
        "DEPTH = 9\n",
        "\n",
        "MODEL_SAVE_AT = MODEL_SAVE_DIR + 'vDCNN/depth_{0}/'.format(DEPTH)\n",
        "if not os.path.exists(MODEL_SAVE_AT):\n",
        "  os.makedirs(MODEL_SAVE_AT)\n",
        "\n",
        "\n",
        "filepath = MODEL_SAVE_AT + 'model' + '.hdf5'\n",
        "logfilepath = MODEL_SAVE_AT + 'logs_' + 'model' + '.csv'\n",
        "\n",
        "reduce_lr_rate=0.2\n",
        "logCallback = CSVLogger(logfilepath, separator=',', append=False)\n",
        "earlyStopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_weights_only=True, verbose=1,\n",
        "                             save_best_only=True, mode='auto')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=reduce_lr_rate, patience=3,\n",
        "                              cooldown=0, min_lr=0.0000000001, verbose=0)\n",
        "\n",
        "callbacks_list = [logCallback, earlyStopping, reduce_lr, checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0d-JCI6Qwwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VDCNN(num_classes=train_Y.shape[1], \n",
        "  depth=DEPTH, \n",
        "  sequence_length=MAX_SENTENCE_LENGTH, \n",
        "  shortcut=False,\n",
        "  pool_type='max', \n",
        "  sorted=False,\n",
        "  use_bias=False)\n",
        "\n",
        "model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtQBQG3AUzQW",
        "colab_type": "text"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DSCZrQMRf7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5cS7SFwUxf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = model.fit(train_X, train_Y, validation_data=(val_X, val_Y), epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "               verbose=1, shuffle=True, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJH1hm4YU1Nr",
        "colab_type": "text"
      },
      "source": [
        "##### Inference and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ttPlnu2U0zK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_X, batch_size=BATCH_SIZE, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUgfowXtsR80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_list = labels_dict.keys()\n",
        "\n",
        "test_Y_max = np.argmax(test_Y, axis=-1)\n",
        "predictions_max = np.argmax(predictions, axis=-1)\n",
        "\n",
        "####################################### CONFUSION MATRIX\n",
        "\n",
        "cm = confusion_matrix(test_Y_max, predictions_max)\n",
        "print('Confusion Matrix:\\n', cm)\n",
        "\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "cm = pd.DataFrame(cm, labels_list, labels_list)\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(cm, annot=True, annot_kws={\"size\": 11}, fmt=\".2f\") # font size\n",
        "plt.show()\n",
        "\n",
        "report_labels = [str(x) for x in labels_dict.values()]\n",
        "#########################################################\n",
        "\n",
        "################################### CLASSIFICATION REPORT\n",
        "print(\"\\n\\nClassification Report\\n\", classification_report(test_Y_max, predictions_max, \n",
        "                                                       labels=list(labels_dict.values()), target_names=report_labels))\n",
        "#########################################################\n",
        "\n",
        "\n",
        "################################# MODEL TRAINING PROGRESS\n",
        "logsdF = pd.read_csv(logfilepath, delimiter=',', header=0, \n",
        "                     names=['epoch', 'accuracy', 'loss', 'val_accuracy', 'val_loss'])\n",
        "ax = plt.gca()\n",
        "\n",
        "logsdF.plot(kind='line', x='epoch', y='accuracy', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='loss', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='val_accuracy', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='val_loss', ax=ax)\n",
        "plt.legend()\n",
        "plt.title('Model Training Progress')\n",
        "plt.show()\n",
        "#########################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpU0fq6-xP09",
        "colab_type": "text"
      },
      "source": [
        "#### Depth = 17 (Convolution Layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74JvlUO1xUCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup callbacks\n",
        "\n",
        "DEPTH = 17\n",
        "\n",
        "MODEL_SAVE_AT = MODEL_SAVE_DIR + 'vDCNN/depth_{0}/'.format(DEPTH)\n",
        "if not os.path.exists(MODEL_SAVE_AT):\n",
        "  os.makedirs(MODEL_SAVE_AT)\n",
        "\n",
        "\n",
        "filepath = MODEL_SAVE_AT + 'model' + '.hdf5'\n",
        "logfilepath = MODEL_SAVE_AT + 'logs_' + 'model' + '.csv'\n",
        "\n",
        "reduce_lr_rate=0.2\n",
        "logCallback = CSVLogger(logfilepath, separator=',', append=False)\n",
        "earlyStopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_weights_only=True, verbose=1,\n",
        "                             save_best_only=True, mode='auto')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=reduce_lr_rate, patience=3,\n",
        "                              cooldown=0, min_lr=0.0000000001, verbose=0)\n",
        "\n",
        "callbacks_list = [logCallback, earlyStopping, reduce_lr, checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIiQ0G5qxnTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VDCNN(num_classes=train_Y.shape[1], \n",
        "  depth=DEPTH, \n",
        "  sequence_length=MAX_SENTENCE_LENGTH, \n",
        "  shortcut=False,\n",
        "  pool_type='max', \n",
        "  sorted=False,\n",
        "  use_bias=False)\n",
        "\n",
        "model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRhtcLV5xzcn",
        "colab_type": "text"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJcS5ox6xr6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yM_x0l9x1R0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = model.fit(train_X, train_Y, validation_data=(val_X, val_Y), epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "               verbose=1, shuffle=True, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxjY8AOBx4ur",
        "colab_type": "text"
      },
      "source": [
        "##### Inference and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UoDzlRqx9_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_X, batch_size=BATCH_SIZE, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A75gP8JPx9v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_list = labels_dict.keys()\n",
        "\n",
        "test_Y_max = np.argmax(test_Y, axis=-1)\n",
        "predictions_max = np.argmax(predictions, axis=-1)\n",
        "\n",
        "####################################### CONFUSION MATRIX\n",
        "\n",
        "cm = confusion_matrix(test_Y_max, predictions_max)\n",
        "print('Confusion Matrix:\\n', cm)\n",
        "\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "cm = pd.DataFrame(cm, labels_list, labels_list)\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(cm, annot=True, annot_kws={\"size\": 11}, fmt=\".2f\") # font size\n",
        "plt.show()\n",
        "\n",
        "report_labels = [str(x) for x in labels_dict.values()]\n",
        "#########################################################\n",
        "\n",
        "################################### CLASSIFICATION REPORT\n",
        "print(\"\\n\\nClassification Report\\n\", classification_report(test_Y_max, predictions_max, \n",
        "                                                       labels=list(labels_dict.values()), target_names=report_labels))\n",
        "#########################################################\n",
        "\n",
        "\n",
        "################################# MODEL TRAINING PROGRESS\n",
        "logsdF = pd.read_csv(logfilepath, delimiter=',', header=0, \n",
        "                     names=['epoch', 'accuracy', 'loss', 'val_accuracy', 'val_loss'])\n",
        "ax = plt.gca()\n",
        "\n",
        "logsdF.plot(kind='line', x='epoch', y='accuracy', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='loss', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='val_accuracy', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='val_loss', ax=ax)\n",
        "plt.legend()\n",
        "plt.title('Model Training Progress')\n",
        "plt.show()\n",
        "#########################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3cCzfIpyD-c",
        "colab_type": "text"
      },
      "source": [
        "#### Depth = 29 (Convolution Layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq2cCudbyH2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup callbacks\n",
        "\n",
        "DEPTH = 29\n",
        "\n",
        "MODEL_SAVE_AT = MODEL_SAVE_DIR + 'vDCNN/depth_{0}/'.format(DEPTH)\n",
        "if not os.path.exists(MODEL_SAVE_AT):\n",
        "  os.makedirs(MODEL_SAVE_AT)\n",
        "\n",
        "\n",
        "filepath = MODEL_SAVE_AT + 'model' + '.hdf5'\n",
        "logfilepath = MODEL_SAVE_AT + 'logs_' + 'model' + '.csv'\n",
        "\n",
        "reduce_lr_rate=0.2\n",
        "logCallback = CSVLogger(logfilepath, separator=',', append=False)\n",
        "earlyStopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_weights_only=True, verbose=1,\n",
        "                             save_best_only=True, mode='auto')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=reduce_lr_rate, patience=3,\n",
        "                              cooldown=0, min_lr=0.0000000001, verbose=0)\n",
        "\n",
        "callbacks_list = [logCallback, earlyStopping, reduce_lr, checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0LFHQxt3U3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VDCNN(num_classes=train_Y.shape[1], \n",
        "  depth=DEPTH, \n",
        "  sequence_length=MAX_SENTENCE_LENGTH, \n",
        "  shortcut=False,\n",
        "  pool_type='max', \n",
        "  sorted=False,\n",
        "  use_bias=False)\n",
        "\n",
        "model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLQ6MaC43ahS",
        "colab_type": "text"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWbHS7A33dgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC3AB5Zs3ezF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = model.fit(train_X, train_Y, validation_data=(val_X, val_Y), epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "               verbose=1, shuffle=True, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcvWSybf3fS4",
        "colab_type": "text"
      },
      "source": [
        "##### Inference and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbF-5luS3j8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_X, batch_size=BATCH_SIZE, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOV5hgFc3jjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_list = labels_dict.keys()\n",
        "\n",
        "test_Y_max = np.argmax(test_Y, axis=-1)\n",
        "predictions_max = np.argmax(predictions, axis=-1)\n",
        "\n",
        "####################################### CONFUSION MATRIX\n",
        "\n",
        "cm = confusion_matrix(test_Y_max, predictions_max)\n",
        "print('Confusion Matrix:\\n', cm)\n",
        "\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "cm = pd.DataFrame(cm, labels_list, labels_list)\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(cm, annot=True, annot_kws={\"size\": 11}, fmt=\".2f\") # font size\n",
        "plt.show()\n",
        "\n",
        "report_labels = [str(x) for x in labels_dict.values()]\n",
        "#########################################################\n",
        "\n",
        "################################### CLASSIFICATION REPORT\n",
        "print(\"\\n\\nClassification Report\\n\", classification_report(test_Y_max, predictions_max, \n",
        "                                                       labels=list(labels_dict.values()), target_names=report_labels))\n",
        "#########################################################\n",
        "\n",
        "\n",
        "################################# MODEL TRAINING PROGRESS\n",
        "logsdF = pd.read_csv(logfilepath, delimiter=',', header=0, \n",
        "                     names=['epoch', 'accuracy', 'loss', 'val_accuracy', 'val_loss'])\n",
        "ax = plt.gca()\n",
        "\n",
        "logsdF.plot(kind='line', x='epoch', y='accuracy', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='loss', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='val_accuracy', ax=ax)\n",
        "logsdF.plot(kind='line', x='epoch', y='val_loss', ax=ax)\n",
        "plt.legend()\n",
        "plt.title('Model Training Progress')\n",
        "plt.show()\n",
        "#########################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rand7a6RZbXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}