{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Topic Detection LDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr20pEssxUT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim\n",
        "import scipy.sparse as sp\n",
        "import scipy.linalg as sparcyLinalg\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGQMR6LfxqYV",
        "colab_type": "code",
        "outputId": "f9b064fe-c985-4379-c30a-c9ca857a39a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAs3fiSiD4oG",
        "colab_type": "code",
        "outputId": "3dc758c7-84e4-4a38-fc55-200f516c62f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_plDjnFyRR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_DIR = '/content/drive/My Drive/tfidf_cosine/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLWae3iZxUUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASETS_PATH = BASE_DIR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F9zQibVxUUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(filename_train, filename_test, filename_val):\n",
        "    \"\"\"\n",
        "    Load train, test and validation sets from file\n",
        "\n",
        "    Args:\n",
        "        filename_train : Name of the file from which the train data is to be loaded\n",
        "        filename_test : Name of the file from which the test data is to be loaded\n",
        "        filename_val : Name of the file from which the validation data is to be loaded\n",
        "    \n",
        "    Returns:\n",
        "        train_tweet_X: list of tweets from train data\n",
        "        train_tweet_Y: list of lables correponding to each tweet from train data\n",
        "        test_tweet_X: list of tweets from test data\n",
        "        test_tweet_Y: list of lables correponding to each tweet from test data\n",
        "        val_tweet_X: list of tweets from validation data\n",
        "        val_tweet_Y: list of lables correponding to each tweet from validation data\n",
        "    \"\"\"\n",
        "    train = pd.read_csv(filename_train)\n",
        "    test = pd.read_csv(filename_test)\n",
        "    val = pd.read_csv(filename_val)\n",
        "    \n",
        "    return train['Text'].tolist(), train['Label'].tolist(), test['Text'].tolist(), test['Label'].tolist(), val['Text'].tolist(), val['Label'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWLE3RuOxUUj",
        "colab_type": "code",
        "outputId": "6d784964-59e3-4def-e302-1cee99bd2ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# LOADING TRAIN, TEST AND VALIDATION SETS\n",
        "\n",
        "train_tweet_X, train_tweet_Y, test_tweet_X, test_tweet_Y, val_tweet_X, val_tweet_Y = load_data(DATASETS_PATH + 'train.csv', DATASETS_PATH + 'test.csv', DATASETS_PATH + 'val.csv')\n",
        "\n",
        "print(\"Train length: \\t\", len(train_tweet_X))\n",
        "print(\"Test length: \\t\", len(test_tweet_X))\n",
        "print(\"Val length: \\t\", len(val_tweet_X))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train length: \t 40231\n",
            "Test length: \t 11833\n",
            "Val length: \t 7100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtiNk8Y3DvTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(tweets):\n",
        "    \"\"\"\n",
        "    Perform preprocessing of the tweets\n",
        "\n",
        "    Args:\n",
        "        tweets : list of tweets\n",
        "    \n",
        "    Returns:\n",
        "        result: preprocessed list of tweets\n",
        "    \"\"\"\n",
        "    #set of stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    \n",
        "    result = []\n",
        "    for tweet in tweets:\n",
        "        \n",
        "        #tokenizing each tweet\n",
        "        tokens = word_tokenize(tweet)\n",
        "        \n",
        "        #removing stopwords and keeping words length greater than 2\n",
        "        stopwords_removed_tokens = []\n",
        "        for word in tokens:\n",
        "            if word not in stop_words and len(word) > 2:\n",
        "                stopwords_removed_tokens.append(word)\n",
        "        \n",
        "        #lemmatization and stemming\n",
        "        lemmatized_tokens = []\n",
        "        for word in stopwords_removed_tokens:\n",
        "          lemmatized_tokens.append(stemmer.stem(lemmatizer.lemmatize(word)))\n",
        "\n",
        "        \n",
        "        result.append(lemmatized_tokens)\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gibNqUDuDyuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREPROCESSING TRAIN, TEST AND VALIDATION TWEETS\n",
        "\n",
        "train_tweet_X = preprocessing(train_tweet_X)\n",
        "test_tweet_X = preprocessing(test_tweet_X)\n",
        "val_tweet_X = preprocessing(val_tweet_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1sFG0AFdkgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#COMBINING TRAIN AND TEST TWEETS\n",
        "\n",
        "combined_tweets_X = []\n",
        "for tweet in train_tweet_X:\n",
        "    combined_tweets_X.append(tweet)\n",
        "for tweet in test_tweet_X:\n",
        "    combined_tweets_X.append(tweet)\n",
        "\n",
        "combined_Y = []\n",
        "for label in train_tweet_Y:\n",
        "    combined_Y.append(label)\n",
        "for label in test_tweet_Y:\n",
        "    combined_Y.append('UNKNOWN')\n",
        "\n",
        "actual_combined_Y = []\n",
        "for label in train_tweet_Y:\n",
        "    actual_combined_Y.append(label)\n",
        "for label in test_tweet_Y:\n",
        "    actual_combined_Y.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkGzHWZtdkmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CREATING ARRAYS CONTAINING INDICES CORREPONDING TO EACH POLITICIANS TWEET IN combined_tweets_X\n",
        "fawadchaudhry_tweets = []\n",
        "maryamnsharif_tweets = []\n",
        "mjibrannasir_tweets = []\n",
        "narendramodi_tweets = []\n",
        "sherryrehman_tweets = []\n",
        "\n",
        "for i, label in enumerate(actual_combined_Y):\n",
        "  if label == 'fawadchaudhry':\n",
        "    fawadchaudhry_tweets.append(combined_tweets_X[i])\n",
        "  elif label == 'maryamnsharif':\n",
        "    maryamnsharif_tweets.append(combined_tweets_X[i])\n",
        "  elif label == 'mjibrannasir':\n",
        "    mjibrannasir_tweets.append(combined_tweets_X[i])\n",
        "  elif label == 'narendramodi':\n",
        "    narendramodi_tweets.append(combined_tweets_X[i])\n",
        "  elif label == 'sherryrehman':\n",
        "    sherryrehman_tweets.append(combined_tweets_X[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDRoF9D0n1GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fawadchaudhry_dictionary = gensim.corpora.Dictionary(fawadchaudhry_tweets)\n",
        "maryamnsharif_dictionary = gensim.corpora.Dictionary(maryamnsharif_tweets)\n",
        "mjibrannasir_dictionary = gensim.corpora.Dictionary(mjibrannasir_tweets)\n",
        "narendramodi_dictionary = gensim.corpora.Dictionary(narendramodi_tweets)\n",
        "sherryrehman_dictionary = gensim.corpora.Dictionary(sherryrehman_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pws_JurhrzCm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "f220b425-1532-4d01-afad-75b3c34db6cc"
      },
      "source": [
        "'''\n",
        "Checking dictionaries created\n",
        "'''\n",
        "count = 0\n",
        "for k, v in fawadchaudhry_dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break\n",
        "\n",
        "count = 0\n",
        "for k, v in maryamnsharif_dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break\n",
        "\n",
        "count = 0\n",
        "for k, v in mjibrannasir_dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break\n",
        "\n",
        "count = 0\n",
        "for k, v in narendramodi_dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break\n",
        "\n",
        "count = 0\n",
        "for k, v in sherryrehman_dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 baloch\n",
            "1 cricket\n",
            "2 jameel\n",
            "3 one\n",
            "4 page\n",
            "5 pakistan\n",
            "6 qndeel\n",
            "7 tariq\n",
            "8 unit\n",
            "9 wct20\n",
            "10 advoc\n",
            "0 instead\n",
            "1 mian\n",
            "2 mosu\n",
            "3 nawaz\n",
            "4 other\n",
            "5 pakistan\n",
            "6 politician\n",
            "7 popular\n",
            "8 rule\n",
            "9 sharif\n",
            "10 abt\n",
            "0 address\n",
            "1 avenu\n",
            "2 coal\n",
            "3 concern\n",
            "4 csr\n",
            "5 develop\n",
            "6 dir\n",
            "7 goranno\n",
            "8 mine\n",
            "9 must\n",
            "10 new\n",
            "0 arm\n",
            "1 day\n",
            "2 donat\n",
            "3 flag\n",
            "4 forc\n",
            "5 generous\n",
            "6 personnel\n",
            "7 rememb\n",
            "8 valour\n",
            "9 welfar\n",
            "10 common\n",
            "0 coverag\n",
            "1 crisi\n",
            "2 expand\n",
            "3 factual\n",
            "4 give\n",
            "5 keep\n",
            "6 minimum\n",
            "7 mute\n",
            "8 pindi\n",
            "9 rise\n",
            "10 rumor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UX518CMsEJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "OPTIONAL STEP\n",
        "Remove very rare and very common words:\n",
        "\n",
        "- words appearing less than 15 times\n",
        "- words appearing in more than 10% of all documents\n",
        "'''\n",
        "# fawadchaudhry_dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)\n",
        "fawadchaudhry_dictionary.filter_extremes(no_below=3)\n",
        "maryamnsharif_dictionary.filter_extremes(no_below=3)\n",
        "mjibrannasir_dictionary.filter_extremes(no_below=3)\n",
        "narendramodi_dictionary.filter_extremes(no_below=3)\n",
        "sherryrehman_dictionary.filter_extremes(no_below=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZtNbqzWn1K8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
        "words and how many times those words appear. Save this to 'bow_corpus'\n",
        "'''\n",
        "fawadchaudhry_bow_corpus = [fawadchaudhry_dictionary.doc2bow(doc) for doc in fawadchaudhry_tweets]\n",
        "maryamnsharif_bow_corpus = [maryamnsharif_dictionary.doc2bow(doc) for doc in maryamnsharif_tweets]\n",
        "mjibrannasir_bow_corpus = [mjibrannasir_dictionary.doc2bow(doc) for doc in mjibrannasir_tweets]\n",
        "narendramodi_bow_corpus = [narendramodi_dictionary.doc2bow(doc) for doc in narendramodi_tweets]\n",
        "sherryrehman_bow_corpus = [sherryrehman_dictionary.doc2bow(doc) for doc in sherryrehman_tweets]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYjA4kVgsUbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "148e3e83-49fe-481c-c7e5-4a1515cd4a91"
      },
      "source": [
        "'''\n",
        "Preview BOW for our sample preprocessed document\n",
        "'''\n",
        "document_num = 20\n",
        "sherryrehman_bow_doc_x = sherryrehman_bow_corpus[document_num]\n",
        "\n",
        "for i in range(len(sherryrehman_bow_doc_x)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(sherryrehman_bow_doc_x[i][0], \n",
        "                                                     sherryrehman_dictionary[sherryrehman_bow_doc_x[i][0]], \n",
        "                                                     sherryrehman_bow_doc_x[i][1]))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 3 (\"give\") appears 1 time.\n",
            "Word 49 (\"spend\") appears 1 time.\n",
            "Word 91 (\"forward\") appears 1 time.\n",
            "Word 97 (\"realli\") appears 1 time.\n",
            "Word 149 (\"better\") appears 1 time.\n",
            "Word 151 (\"child\") appears 1 time.\n",
            "Word 157 (\"neemtreeiftar\") appears 1 time.\n",
            "Word 225 (\"abil\") appears 1 time.\n",
            "Word 226 (\"access\") appears 1 time.\n",
            "Word 227 (\"amaz\") appears 1 time.\n",
            "Word 228 (\"hope\") appears 1 time.\n",
            "Word 229 (\"look\") appears 1 time.\n",
            "Word 230 (\"opportun\") appears 1 time.\n",
            "Word 231 (\"potenti\") appears 1 time.\n",
            "Word 232 (\"realis\") appears 1 time.\n",
            "Word 233 (\"thank\") appears 1 time.\n",
            "Word 234 (\"time\") appears 1 time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF3Vi8V3n1Sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fawadchaudhry_model =  gensim.models.LdaMulticore(fawadchaudhry_bow_corpus, num_topics = 8, id2word = fawadchaudhry_dictionary, passes = 10, workers = 2)\n",
        "maryamnsharif_model =  gensim.models.LdaMulticore(maryamnsharif_bow_corpus, num_topics = 8, id2word = maryamnsharif_dictionary, passes = 10, workers = 2)\n",
        "mjibrannasir_model =  gensim.models.LdaMulticore(mjibrannasir_bow_corpus, num_topics = 8, id2word = mjibrannasir_dictionary, passes = 10, workers = 2)\n",
        "narendramodi_model =  gensim.models.LdaMulticore(narendramodi_bow_corpus, num_topics = 8, id2word = narendramodi_dictionary, passes = 10, workers = 2)\n",
        "sherryrehman_model =  gensim.models.LdaMulticore(sherryrehman_bow_corpus, num_topics = 8, id2word = sherryrehman_dictionary, passes = 10, workers = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxwXeXmEo8-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "285828fe-cd35-48b6-b80e-c7193182e120"
      },
      "source": [
        "'''\n",
        "For each topic, we will explore the words occuring in that topic and its relative weight\n",
        "'''\n",
        "print(\"fawadchaudhry\")\n",
        "for idx, topic in fawadchaudhry_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")\n",
        "print(\"maryamnsharif\")\n",
        "for idx, topic in maryamnsharif_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")\n",
        "print(\"mjibrannasir\")\n",
        "for idx, topic in mjibrannasir_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")\n",
        "print(\"narendramodi\")\n",
        "for idx, topic in narendramodi_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")\n",
        "print(\"sherryrehman\")\n",
        "for idx, topic in sherryrehman_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fawadchaudhry\n",
            "Topic: 0 \n",
            "Words: 0.053*\"hai\" + 0.024*\"tou\" + 0.019*\"hein\" + 0.019*\"mein\" + 0.019*\"aur\" + 0.018*\"kia\" + 0.017*\"app\" + 0.016*\"nahi\" + 0.013*\"nai\" + 0.013*\"yeah\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.017*\"pak\" + 0.010*\"must\" + 0.009*\"thi\" + 0.007*\"ppp\" + 0.007*\"world\" + 0.007*\"polit\" + 0.006*\"peopl\" + 0.006*\"knw\" + 0.005*\"india\" + 0.005*\"like\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.014*\"pti\" + 0.011*\"ppp\" + 0.011*\"one\" + 0.009*\"pak\" + 0.009*\"kill\" + 0.009*\"support\" + 0.008*\"must\" + 0.008*\"mqm\" + 0.007*\"pmln\" + 0.007*\"tht\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.011*\"cricket\" + 0.009*\"one\" + 0.007*\"pti\" + 0.007*\"cong\" + 0.006*\"report\" + 0.005*\"old\" + 0.005*\"provinc\" + 0.005*\"state\" + 0.005*\"parti\" + 0.005*\"wish\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.031*\"pak\" + 0.009*\"medium\" + 0.008*\"see\" + 0.007*\"even\" + 0.007*\"pti\" + 0.007*\"armi\" + 0.006*\"want\" + 0.006*\"state\" + 0.006*\"use\" + 0.006*\"like\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.013*\"tht\" + 0.013*\"elect\" + 0.011*\"case\" + 0.009*\"judg\" + 0.009*\"pak\" + 0.009*\"shld\" + 0.008*\"cjp\" + 0.008*\"need\" + 0.007*\"court\" + 0.006*\"right\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.011*\"govt\" + 0.010*\"peopl\" + 0.009*\"support\" + 0.009*\"india\" + 0.008*\"frm\" + 0.008*\"pti\" + 0.007*\"extremist\" + 0.007*\"thi\" + 0.007*\"shld\" + 0.007*\"see\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.016*\"govt\" + 0.014*\"pak\" + 0.010*\"hai\" + 0.010*\"tht\" + 0.008*\"peopl\" + 0.006*\"make\" + 0.006*\"janay\" + 0.006*\"think\" + 0.006*\"fail\" + 0.005*\"well\"\n",
            "\n",
            "\n",
            "maryamnsharif\n",
            "Topic: 0 \n",
            "Words: 0.027*\"minist\" + 0.023*\"prime\" + 0.012*\"repli\" + 0.008*\"pmln\" + 0.008*\"tweet\" + 0.007*\"youth\" + 0.007*\"health\" + 0.007*\"team\" + 0.007*\"sharif\" + 0.006*\"meet\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.009*\"thank\" + 0.009*\"amp\" + 0.009*\"govt\" + 0.008*\"meet\" + 0.007*\"punjab\" + 0.007*\"know\" + 0.007*\"leader\" + 0.006*\"pls\" + 0.006*\"tweet\" + 0.006*\"respect\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.024*\"pmln\" + 0.012*\"see\" + 0.012*\"sharif\" + 0.011*\"thank\" + 0.010*\"nawaz\" + 0.009*\"elect\" + 0.008*\"need\" + 0.007*\"hai\" + 0.006*\"amp\" + 0.006*\"new\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.015*\"amp\" + 0.010*\"make\" + 0.009*\"ppl\" + 0.008*\"give\" + 0.008*\"pmln\" + 0.007*\"youth\" + 0.007*\"question\" + 0.007*\"thank\" + 0.007*\"good\" + 0.007*\"pakistan\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.017*\"amp\" + 0.009*\"pmln\" + 0.008*\"one\" + 0.008*\"time\" + 0.008*\"tri\" + 0.006*\"see\" + 0.006*\"get\" + 0.006*\"never\" + 0.006*\"power\" + 0.006*\"pls\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.026*\"allah\" + 0.020*\"may\" + 0.020*\"bless\" + 0.016*\"pmln\" + 0.016*\"thank\" + 0.015*\"amp\" + 0.012*\"happi\" + 0.010*\"leader\" + 0.010*\"great\" + 0.009*\"ameen\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.011*\"amp\" + 0.010*\"pakistan\" + 0.008*\"scheme\" + 0.008*\"back\" + 0.008*\"polit\" + 0.007*\"loan\" + 0.007*\"nawaz\" + 0.007*\"pak\" + 0.006*\"sharif\" + 0.006*\"one\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.040*\"sharif\" + 0.036*\"nawaz\" + 0.014*\"pmln\" + 0.014*\"pml\" + 0.011*\"know\" + 0.010*\"vote\" + 0.010*\"say\" + 0.010*\"pti\" + 0.008*\"amp\" + 0.006*\"mian\"\n",
            "\n",
            "\n",
            "mjibrannasir\n",
            "Topic: 0 \n",
            "Words: 0.009*\"pakistan\" + 0.008*\"kashmir\" + 0.008*\"law\" + 0.008*\"india\" + 0.008*\"right\" + 0.007*\"live\" + 0.007*\"discuss\" + 0.007*\"abdul\" + 0.006*\"need\" + 0.006*\"aziz\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.013*\"amp\" + 0.011*\"thank\" + 0.010*\"pakistani\" + 0.009*\"pleas\" + 0.008*\"need\" + 0.008*\"camp\" + 0.007*\"team\" + 0.007*\"help\" + 0.007*\"school\" + 0.007*\"support\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.013*\"govt\" + 0.009*\"pakistan\" + 0.008*\"ban\" + 0.007*\"also\" + 0.007*\"aswj\" + 0.006*\"elect\" + 0.006*\"parti\" + 0.005*\"polit\" + 0.005*\"respons\" + 0.005*\"minist\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.038*\"hai\" + 0.033*\"kay\" + 0.026*\"main\" + 0.025*\"hain\" + 0.024*\"nahi\" + 0.023*\"aur\" + 0.023*\"aap\" + 0.016*\"say\" + 0.015*\"toh\" + 0.014*\"bhi\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.021*\"support\" + 0.013*\"pti\" + 0.012*\"day\" + 0.010*\"amp\" + 0.010*\"post\" + 0.010*\"pakistan\" + 0.010*\"karachi\" + 0.007*\"ppp\" + 0.007*\"mqm\" + 0.007*\"world\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.009*\"protest\" + 0.008*\"pakistan\" + 0.008*\"one\" + 0.007*\"polit\" + 0.007*\"attack\" + 0.005*\"right\" + 0.005*\"amp\" + 0.005*\"peopl\" + 0.004*\"thank\" + 0.004*\"muslim\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.022*\"main\" + 0.014*\"hai\" + 0.010*\"food\" + 0.010*\"humqadam\" + 0.010*\"awam\" + 0.009*\"pack\" + 0.008*\"aur\" + 0.008*\"hussain\" + 0.007*\"box\" + 0.007*\"kay\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.018*\"polic\" + 0.017*\"case\" + 0.016*\"court\" + 0.009*\"today\" + 0.009*\"student\" + 0.007*\"govt\" + 0.007*\"amp\" + 0.007*\"famili\" + 0.007*\"fir\" + 0.007*\"sindh\"\n",
            "\n",
            "\n",
            "narendramodi\n",
            "Topic: 0 \n",
            "Words: 0.026*\"share\" + 0.022*\"look\" + 0.022*\"programm\" + 0.019*\"forward\" + 0.018*\"interact\" + 0.017*\"address\" + 0.013*\"join\" + 0.013*\"meet\" + 0.012*\"day\" + 0.012*\"today\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.023*\"congratul\" + 0.022*\"peopl\" + 0.016*\"bjp\" + 0.014*\"ralli\" + 0.012*\"india\" + 0.012*\"proud\" + 0.010*\"elect\" + 0.010*\"team\" + 0.010*\"win\" + 0.009*\"today\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.057*\"india\" + 0.016*\"discuss\" + 0.015*\"tie\" + 0.015*\"meet\" + 0.014*\"presid\" + 0.013*\"relat\" + 0.011*\"nation\" + 0.011*\"cooper\" + 0.010*\"peopl\" + 0.010*\"talk\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.029*\"wish\" + 0.025*\"life\" + 0.022*\"shri\" + 0.021*\"may\" + 0.020*\"long\" + 0.018*\"greet\" + 0.016*\"good\" + 0.016*\"birthday\" + 0.014*\"best\" + 0.013*\"health\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.013*\"develop\" + 0.013*\"project\" + 0.013*\"india\" + 0.011*\"effort\" + 0.011*\"toward\" + 0.009*\"bharat\" + 0.008*\"mission\" + 0.007*\"foundat\" + 0.007*\"initi\" + 0.007*\"inaugur\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.011*\"share\" + 0.010*\"life\" + 0.010*\"make\" + 0.009*\"india\" + 0.009*\"work\" + 0.009*\"youth\" + 0.008*\"youngster\" + 0.008*\"young\" + 0.007*\"child\" + 0.007*\"world\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.021*\"govern\" + 0.020*\"congress\" + 0.018*\"peopl\" + 0.015*\"farmer\" + 0.011*\"state\" + 0.010*\"famili\" + 0.009*\"polit\" + 0.009*\"need\" + 0.009*\"corrupt\" + 0.008*\"nda\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.026*\"india\" + 0.019*\"nation\" + 0.014*\"peopl\" + 0.012*\"work\" + 0.010*\"inspir\" + 0.010*\"life\" + 0.010*\"indian\" + 0.009*\"rememb\" + 0.009*\"day\" + 0.009*\"tribut\"\n",
            "\n",
            "\n",
            "sherryrehman\n",
            "Topic: 0 \n",
            "Words: 0.013*\"pakistan\" + 0.009*\"need\" + 0.007*\"polic\" + 0.006*\"pakistani\" + 0.006*\"govt\" + 0.006*\"mani\" + 0.005*\"get\" + 0.005*\"take\" + 0.005*\"see\" + 0.005*\"fight\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.011*\"govt\" + 0.009*\"pakistan\" + 0.008*\"senat\" + 0.008*\"water\" + 0.007*\"need\" + 0.006*\"life\" + 0.006*\"one\" + 0.006*\"sindh\" + 0.005*\"woman\" + 0.005*\"nation\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.020*\"pakistan\" + 0.008*\"need\" + 0.007*\"must\" + 0.006*\"india\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"year\" + 0.005*\"say\" + 0.005*\"one\" + 0.005*\"afghanistan\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.025*\"pakistan\" + 0.012*\"woman\" + 0.012*\"india\" + 0.011*\"today\" + 0.008*\"pak\" + 0.007*\"good\" + 0.007*\"look\" + 0.007*\"time\" + 0.006*\"like\" + 0.006*\"new\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.018*\"pakistan\" + 0.007*\"peac\" + 0.007*\"say\" + 0.007*\"may\" + 0.006*\"day\" + 0.006*\"new\" + 0.006*\"govt\" + 0.006*\"today\" + 0.006*\"india\" + 0.005*\"come\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.015*\"woman\" + 0.011*\"right\" + 0.010*\"pakistan\" + 0.009*\"kill\" + 0.008*\"famili\" + 0.007*\"need\" + 0.007*\"condemn\" + 0.006*\"law\" + 0.006*\"attack\" + 0.006*\"child\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.013*\"pakistan\" + 0.012*\"time\" + 0.012*\"good\" + 0.010*\"mani\" + 0.008*\"thank\" + 0.008*\"chang\" + 0.008*\"day\" + 0.008*\"see\" + 0.007*\"peopl\" + 0.006*\"well\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.028*\"pakistan\" + 0.011*\"senat\" + 0.010*\"govt\" + 0.010*\"bill\" + 0.007*\"ppp\" + 0.007*\"committe\" + 0.007*\"right\" + 0.006*\"today\" + 0.005*\"elect\" + 0.005*\"polici\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM8DtCSgs4H-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a186a924-fdd5-40cc-b6d3-033e33cb2f39"
      },
      "source": [
        "'''\n",
        "For each topic, we will explore the words occuring in that topic and its relative weight\n",
        "'''\n",
        "print(\"fawadchaudhry\")\n",
        "for idx, topic in fawadchaudhry_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")\n",
        "print(\"maryamnsharif\")\n",
        "for idx, topic in maryamnsharif_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")\n",
        "print(\"mjibrannasir\")\n",
        "for idx, topic in mjibrannasir_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")\n",
        "print(\"narendramodi\")\n",
        "for idx, topic in narendramodi_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")\n",
        "print(\"sherryrehman\")\n",
        "for idx, topic in sherryrehman_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fawadchaudhry\n",
            "Topic: 0 \n",
            "Words: 0.049*\"hai\" + 0.031*\"hein\" + 0.017*\"app\" + 0.015*\"mein\" + 0.014*\"yeah\" + 0.012*\"tou\" + 0.011*\"medium\" + 0.011*\"aur\" + 0.010*\"nai\" + 0.009*\"bhai\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.020*\"kill\" + 0.015*\"say\" + 0.011*\"right\" + 0.010*\"thi\" + 0.010*\"taliban\" + 0.010*\"woman\" + 0.010*\"drone\" + 0.008*\"see\" + 0.008*\"support\" + 0.008*\"condemn\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.019*\"pti\" + 0.019*\"govt\" + 0.019*\"ppp\" + 0.013*\"tht\" + 0.012*\"pmln\" + 0.009*\"state\" + 0.009*\"must\" + 0.009*\"thi\" + 0.009*\"polit\" + 0.008*\"support\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.021*\"tht\" + 0.018*\"shld\" + 0.013*\"must\" + 0.011*\"hve\" + 0.011*\"decis\" + 0.010*\"day\" + 0.009*\"knw\" + 0.008*\"state\" + 0.008*\"make\" + 0.007*\"ask\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.018*\"peopl\" + 0.011*\"india\" + 0.010*\"like\" + 0.009*\"world\" + 0.009*\"armi\" + 0.009*\"state\" + 0.008*\"pti\" + 0.008*\"wht\" + 0.007*\"ppp\" + 0.007*\"best\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.013*\"case\" + 0.012*\"judg\" + 0.011*\"court\" + 0.011*\"law\" + 0.011*\"time\" + 0.011*\"one\" + 0.010*\"sharif\" + 0.010*\"need\" + 0.010*\"cjp\" + 0.010*\"plz\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.012*\"use\" + 0.011*\"govt\" + 0.011*\"shld\" + 0.009*\"frm\" + 0.009*\"pti\" + 0.009*\"make\" + 0.009*\"thi\" + 0.008*\"hai\" + 0.008*\"see\" + 0.008*\"tht\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.050*\"hai\" + 0.035*\"tou\" + 0.026*\"kia\" + 0.023*\"aur\" + 0.020*\"mein\" + 0.020*\"nahi\" + 0.013*\"app\" + 0.011*\"nai\" + 0.011*\"ker\" + 0.011*\"hain\"\n",
            "\n",
            "\n",
            "maryamnsharif\n",
            "Topic: 0 \n",
            "Words: 0.029*\"amp\" + 0.016*\"govt\" + 0.015*\"need\" + 0.015*\"follow\" + 0.013*\"also\" + 0.012*\"help\" + 0.012*\"one\" + 0.011*\"yes\" + 0.011*\"polit\" + 0.011*\"say\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.066*\"sharif\" + 0.057*\"nawaz\" + 0.016*\"pti\" + 0.013*\"hai\" + 0.012*\"mian\" + 0.012*\"khan\" + 0.012*\"pml\" + 0.011*\"visit\" + 0.011*\"pls\" + 0.011*\"main\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.023*\"amp\" + 0.020*\"pml\" + 0.019*\"know\" + 0.014*\"pti\" + 0.014*\"polit\" + 0.013*\"vote\" + 0.012*\"lie\" + 0.012*\"fact\" + 0.011*\"day\" + 0.009*\"yes\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.040*\"minist\" + 0.036*\"prime\" + 0.030*\"sharif\" + 0.025*\"nawaz\" + 0.014*\"amp\" + 0.011*\"question\" + 0.009*\"youth\" + 0.009*\"peopl\" + 0.009*\"answer\" + 0.008*\"pak\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.027*\"one\" + 0.011*\"thank\" + 0.011*\"time\" + 0.009*\"amp\" + 0.009*\"realli\" + 0.009*\"good\" + 0.009*\"come\" + 0.008*\"read\" + 0.008*\"like\" + 0.007*\"inshaallah\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.021*\"pakistan\" + 0.021*\"well\" + 0.018*\"see\" + 0.015*\"good\" + 0.015*\"wish\" + 0.012*\"done\" + 0.011*\"pml\" + 0.011*\"medium\" + 0.010*\"parti\" + 0.010*\"tweet\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.041*\"allah\" + 0.035*\"bless\" + 0.029*\"may\" + 0.025*\"thank\" + 0.020*\"god\" + 0.019*\"happi\" + 0.014*\"work\" + 0.013*\"leader\" + 0.013*\"ameen\" + 0.012*\"day\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.033*\"amp\" + 0.013*\"thank\" + 0.011*\"busi\" + 0.011*\"get\" + 0.010*\"use\" + 0.010*\"make\" + 0.009*\"would\" + 0.009*\"polit\" + 0.009*\"show\" + 0.009*\"support\"\n",
            "\n",
            "\n",
            "mjibrannasir\n",
            "Topic: 0 \n",
            "Words: 0.025*\"polic\" + 0.015*\"kill\" + 0.012*\"famili\" + 0.009*\"report\" + 0.009*\"attack\" + 0.008*\"life\" + 0.008*\"hospit\" + 0.008*\"case\" + 0.007*\"offic\" + 0.007*\"amp\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.024*\"govt\" + 0.015*\"ban\" + 0.015*\"aswj\" + 0.013*\"sindh\" + 0.013*\"ppp\" + 0.012*\"elect\" + 0.012*\"pti\" + 0.010*\"today\" + 0.010*\"outfit\" + 0.009*\"pmln\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.022*\"court\" + 0.015*\"case\" + 0.009*\"parti\" + 0.009*\"take\" + 0.008*\"abdul\" + 0.008*\"govt\" + 0.008*\"aziz\" + 0.008*\"action\" + 0.008*\"polit\" + 0.007*\"file\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.017*\"pakistan\" + 0.015*\"thank\" + 0.014*\"student\" + 0.012*\"today\" + 0.009*\"pakistani\" + 0.009*\"need\" + 0.008*\"amp\" + 0.007*\"support\" + 0.007*\"famili\" + 0.006*\"hope\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.015*\"right\" + 0.011*\"make\" + 0.009*\"law\" + 0.009*\"one\" + 0.008*\"public\" + 0.007*\"also\" + 0.007*\"say\" + 0.007*\"person\" + 0.007*\"polit\" + 0.006*\"minist\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.056*\"hai\" + 0.046*\"kay\" + 0.044*\"main\" + 0.034*\"aur\" + 0.034*\"nahi\" + 0.033*\"hain\" + 0.030*\"aap\" + 0.021*\"say\" + 0.021*\"bhi\" + 0.020*\"toh\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.040*\"support\" + 0.021*\"day\" + 0.020*\"amp\" + 0.019*\"karachi\" + 0.018*\"camp\" + 0.017*\"humqadam\" + 0.015*\"relief\" + 0.014*\"water\" + 0.014*\"idp\" + 0.013*\"food\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.018*\"amp\" + 0.013*\"pakistan\" + 0.009*\"live\" + 0.009*\"one\" + 0.008*\"protest\" + 0.008*\"right\" + 0.008*\"dont\" + 0.008*\"like\" + 0.008*\"view\" + 0.007*\"tonight\"\n",
            "\n",
            "\n",
            "narendramodi\n",
            "Topic: 0 \n",
            "Words: 0.020*\"toward\" + 0.015*\"inspir\" + 0.015*\"rememb\" + 0.014*\"life\" + 0.014*\"tribut\" + 0.013*\"nation\" + 0.012*\"work\" + 0.012*\"effort\" + 0.011*\"bharat\" + 0.011*\"contribut\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.030*\"famili\" + 0.013*\"condol\" + 0.013*\"demis\" + 0.011*\"spoke\" + 0.011*\"peac\" + 0.011*\"due\" + 0.010*\"life\" + 0.009*\"situat\" + 0.009*\"thought\" + 0.009*\"chang\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.044*\"life\" + 0.040*\"wish\" + 0.035*\"may\" + 0.033*\"long\" + 0.032*\"greet\" + 0.030*\"good\" + 0.026*\"birthday\" + 0.025*\"shri\" + 0.025*\"pray\" + 0.023*\"bless\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.024*\"meet\" + 0.020*\"tie\" + 0.019*\"visit\" + 0.019*\"presid\" + 0.019*\"look\" + 0.017*\"forward\" + 0.016*\"nation\" + 0.016*\"interact\" + 0.014*\"discuss\" + 0.014*\"relat\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.023*\"gujarat\" + 0.017*\"share\" + 0.017*\"thank\" + 0.013*\"farmer\" + 0.013*\"take\" + 0.013*\"day\" + 0.010*\"new\" + 0.009*\"programm\" + 0.009*\"part\" + 0.009*\"today\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.027*\"congratul\" + 0.020*\"indian\" + 0.018*\"proud\" + 0.014*\"team\" + 0.012*\"win\" + 0.012*\"nation\" + 0.011*\"woman\" + 0.011*\"success\" + 0.009*\"best\" + 0.009*\"make\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.024*\"govern\" + 0.020*\"develop\" + 0.015*\"sector\" + 0.014*\"work\" + 0.012*\"project\" + 0.012*\"state\" + 0.009*\"transform\" + 0.009*\"progress\" + 0.008*\"also\" + 0.008*\"improv\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.026*\"congress\" + 0.021*\"bjp\" + 0.019*\"ralli\" + 0.012*\"polit\" + 0.012*\"today\" + 0.012*\"elect\" + 0.011*\"develop\" + 0.010*\"share\" + 0.010*\"vote\" + 0.010*\"state\"\n",
            "\n",
            "\n",
            "sherryrehman\n",
            "Topic: 0 \n",
            "Words: 0.027*\"woman\" + 0.012*\"need\" + 0.010*\"good\" + 0.010*\"make\" + 0.009*\"talk\" + 0.009*\"right\" + 0.009*\"may\" + 0.008*\"new\" + 0.008*\"kashmir\" + 0.008*\"one\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.016*\"govt\" + 0.013*\"time\" + 0.010*\"need\" + 0.009*\"well\" + 0.009*\"like\" + 0.008*\"medium\" + 0.007*\"want\" + 0.007*\"done\" + 0.007*\"see\" + 0.007*\"peopl\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.018*\"thank\" + 0.018*\"mani\" + 0.012*\"need\" + 0.009*\"yes\" + 0.009*\"friend\" + 0.008*\"life\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"day\" + 0.007*\"think\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.020*\"govt\" + 0.013*\"senat\" + 0.011*\"bill\" + 0.011*\"parliament\" + 0.011*\"woman\" + 0.010*\"need\" + 0.008*\"get\" + 0.008*\"sindh\" + 0.007*\"ppp\" + 0.007*\"new\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.020*\"india\" + 0.014*\"right\" + 0.011*\"afghanistan\" + 0.010*\"today\" + 0.010*\"war\" + 0.008*\"govt\" + 0.008*\"ppp\" + 0.007*\"talk\" + 0.007*\"polici\" + 0.007*\"new\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.016*\"day\" + 0.013*\"year\" + 0.013*\"peopl\" + 0.009*\"last\" + 0.009*\"one\" + 0.009*\"time\" + 0.009*\"new\" + 0.008*\"karachi\" + 0.008*\"water\" + 0.008*\"like\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.018*\"attack\" + 0.012*\"famili\" + 0.012*\"kill\" + 0.011*\"state\" + 0.011*\"prayer\" + 0.009*\"good\" + 0.009*\"terror\" + 0.008*\"condemn\" + 0.008*\"climat\" + 0.008*\"strong\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.015*\"peac\" + 0.008*\"great\" + 0.008*\"today\" + 0.008*\"pakistani\" + 0.007*\"make\" + 0.007*\"good\" + 0.007*\"nation\" + 0.007*\"power\" + 0.007*\"congratul\" + 0.007*\"let\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtwFywDsm--b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#INFERRING TOPICS FROM THE OUTPUT ABOVE:\n",
        "\n",
        "#FAWAD CHAUDHRY\n",
        "# Topic 0: roman urdu words\n",
        "# Topic 1: terrorists\n",
        "# Topic 2: political parties\n",
        "# Topic 3: decisiveness\n",
        "# Topic 4: india army\n",
        "# Topic 5: judiciary\n",
        "# Topic 6: pti govt\n",
        "# Topic 7:\n",
        "\n",
        "#MARYAM NAWAZ\n",
        "# Topic 0: govt\n",
        "# Topic 1: nawaz sharif meet pti\n",
        "# Topic 2: election\n",
        "# Topic 3: prime minister nawaz sharif press conference(meeting people answer questions)\n",
        "# Topic 4: thanking people\n",
        "# Topic 5: pml wishing well for pakistan \n",
        "# Topic 6: thanking god\n",
        "# Topic 7: businessman support for political party\n",
        "\n",
        "#M JIBRAN NASIR\n",
        "# Topic 0: case where police kills people\n",
        "# Topic 1: elections\n",
        "# Topic 2: court case abdul aziz\n",
        "# Topic 3: pakistani family support student\n",
        "# Topic 4: law making\n",
        "# Topic 5: roman urdu words\n",
        "# Topic 6: support karachi with relief\n",
        "# Topic 7:protest\n",
        "\n",
        "#MODI\n",
        "# Topic 0: inspire effort for contribution to bharat\n",
        "# Topic 1: peace\n",
        "# Topic 2: wishing people\n",
        "# Topic 3: meeting the president\n",
        "# Topic 4: thanking farmers in gujrat\n",
        "# Topic 5: congratulating indian team\n",
        "# Topic 6: \n",
        "# Topic 7:\n",
        "\n",
        "#SHERRY REHMAN\n",
        "# Topic 0: \n",
        "# Topic 1: \n",
        "# Topic 2: \n",
        "# Topic 3: \n",
        "# Topic 4: \n",
        "# Topic 5: \n",
        "# Topic 6: \n",
        "# Topic 7:"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}