{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_PATH = '../dataset/5_processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    return data['Text'].tolist(), data['Label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: \t 59164\n"
     ]
    }
   ],
   "source": [
    "# LOADING DATASET\n",
    "\n",
    "data_X, data_Y = load_data(DATASETS_PATH + 'dataset.csv')\n",
    "\n",
    "print(\"Data length: \\t\", len(data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(tweets):\n",
    "    \"\"\"\n",
    "    Perform preprocessing of the tweets\n",
    "\n",
    "    Args:\n",
    "        tweets : list of tweets\n",
    "    \n",
    "    Returns:\n",
    "        result: preprocessed list of tweets\n",
    "    \"\"\"\n",
    "    #set of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    result = []\n",
    "    for tweet in tweets:\n",
    "        \n",
    "        #tokenizing each tweet\n",
    "        tokens = word_tokenize(tweet)\n",
    "        \n",
    "        #removing stopwords\n",
    "        stopwords_removed_tokens = []\n",
    "        for word in tokens:\n",
    "            if word not in stop_words:\n",
    "                stopwords_removed_tokens.append(word)\n",
    "        \n",
    "        result.append(stopwords_removed_tokens)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = preprocessing(data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fawadchaudhry = []\n",
    "narendramodi = []\n",
    "sherryrehman = []\n",
    "mjibrannasir = []\n",
    "maryamnsharif = []\n",
    "\n",
    "for i, tweet in enumerate(data_X):\n",
    "    if data_Y[i] == 'fawadchaudhry':\n",
    "        fawadchaudhry.append(tweet)\n",
    "    elif data_Y[i] == 'narendramodi':\n",
    "        narendramodi.append(tweet)\n",
    "    elif data_Y[i] == 'sherryrehman':\n",
    "        sherryrehman.append(tweet)\n",
    "    elif data_Y[i] == 'mjibrannasir':\n",
    "        mjibrannasir.append(tweet)\n",
    "    elif data_Y[i] == 'maryamnsharif':\n",
    "        maryamnsharif.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fawadchaudhry_tokens = []\n",
    "narendramodi_tokens = []\n",
    "sherryrehman_tokens = []\n",
    "mjibrannasir_tokens = []\n",
    "maryamnsharif_tokens = []\n",
    "\n",
    "for tweet in fawadchaudhry:\n",
    "    for word in tweet:\n",
    "        fawadchaudhry_tokens.append(word)\n",
    "        \n",
    "for tweet in narendramodi:\n",
    "    for word in tweet:\n",
    "        narendramodi_tokens.append(word)\n",
    "        \n",
    "for tweet in sherryrehman:\n",
    "    for word in tweet:\n",
    "        sherryrehman_tokens.append(word)\n",
    "        \n",
    "for tweet in mjibrannasir:\n",
    "    for word in tweet:\n",
    "        mjibrannasir_tokens.append(word)\n",
    "        \n",
    "for tweet in maryamnsharif:\n",
    "    for word in tweet:\n",
    "        maryamnsharif_tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count_unique(tokens):\n",
    "    \"\"\" Total count of unique tokens\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "        count : int -- number of total unique tokens (integer)\n",
    "    \"\"\"\n",
    "    unique_count = 0\n",
    "    unique_count = len(set(tokens))\n",
    "    \n",
    "    return unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawadchaudhry unique tokens:  19045\n",
      "narendramodi unique tokens:  17138\n",
      "sherryrehman unique tokens:  21150\n",
      "mjibrannasir unique tokens:  21382\n",
      "maryamnsharif unique tokens:  16621\n"
     ]
    }
   ],
   "source": [
    "fawadchaudhry_tokens_unique = token_count_unique(fawadchaudhry_tokens)\n",
    "print(\"fawadchaudhry unique tokens: \", fawadchaudhry_tokens_unique)\n",
    "narendramodi_tokens_unique = token_count_unique(narendramodi_tokens)\n",
    "print(\"narendramodi unique tokens: \", narendramodi_tokens_unique)\n",
    "sherryrehman_tokens_unique = token_count_unique(sherryrehman_tokens)\n",
    "print(\"sherryrehman unique tokens: \", sherryrehman_tokens_unique)\n",
    "mjibrannasir_tokens_unique = token_count_unique(mjibrannasir_tokens)\n",
    "print(\"mjibrannasir unique tokens: \", mjibrannasir_tokens_unique)\n",
    "maryamnsharif_tokens_unique = token_count_unique(maryamnsharif_tokens)\n",
    "print(\"maryamnsharif unique tokens: \", maryamnsharif_tokens_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP] *",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
