{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:43.704937Z",
     "start_time": "2020-05-10T15:33:43.701036Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:43.716501Z",
     "start_time": "2020-05-10T15:33:43.709458Z"
    }
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "MIN_LENGTH = 40\n",
    "\n",
    "DATASET_PREFIX = '../dataset/'\n",
    "ENGLISH_PATH = DATASET_PREFIX + '1_english/'\n",
    "URLS_MENTIONS_REMOVED_PATH = DATASET_PREFIX + '2_filtered_urls_mentions/'\n",
    "MIN_LENGTH_PATH = DATASET_PREFIX + '3_length_greater_than_{0}/'.format(MIN_LENGTH)\n",
    "PREPROCESSED_PATH = DATASET_PREFIX + '4_pre-processed/'\n",
    "PROCESSED_PATH = DATASET_PREFIX + '5_processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:43.729070Z",
     "start_time": "2020-05-10T15:33:43.719335Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_file_pairs(files_dir):\n",
    "    files = glob.glob(files_dir + '*.csv')\n",
    "    file_pairs = []\n",
    "\n",
    "    for each in files:\n",
    "        file_name = each.split('/')[-1:][0]\n",
    "\n",
    "        dF = pd.read_csv(each, usecols=['Text'])\n",
    "        dF['Text'] = dF['Text'].astype('str')\n",
    "        file_pairs.append((file_name, dF))\n",
    "        print('filename: {0}'.format(each))\n",
    "        print('size:     {0}'.format(dF.size))\n",
    "        print('----------------------------------------------------\\n')\n",
    "    \n",
    "    return file_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out URLs and @ Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:43.902328Z",
     "start_time": "2020-05-10T15:33:43.731313Z"
    }
   },
   "outputs": [],
   "source": [
    "english_file_pairs = get_file_pairs(ENGLISH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:45.384793Z",
     "start_time": "2020-05-10T15:33:43.904129Z"
    }
   },
   "outputs": [],
   "source": [
    "# URLS_REMOVED_PATH\n",
    "regex_urls = r'(https?://.+\\b\\/?|.{3}\\.twitter.+\\b)'\n",
    "regex_mentions = r'@\\w+'\n",
    "regex = re.compile(regex_urls)\n",
    "\n",
    "if not os.path.exists(URLS_MENTIONS_REMOVED_PATH):\n",
    "    os.makedirs(URLS_MENTIONS_REMOVED_PATH)\n",
    "\n",
    "for each in english_file_pairs:\n",
    "    filename = each[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = URLS_MENTIONS_REMOVED_PATH + filename\n",
    "\n",
    "    # remove URLs\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(regex_urls, ' '))\n",
    "    # remove mentions\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(regex_mentions, ' '))\n",
    "    # save\n",
    "    dF.to_csv(output_file, index=False, columns=['Text'])\n",
    "    print(dF, '\\n----------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for MIN_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:45.560788Z",
     "start_time": "2020-05-10T15:33:45.387166Z"
    }
   },
   "outputs": [],
   "source": [
    "url_filtered_file_pairs = get_file_pairs(URLS_MENTIONS_REMOVED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:46.251494Z",
     "start_time": "2020-05-10T15:33:45.564725Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(MIN_LENGTH_PATH):\n",
    "    os.makedirs(MIN_LENGTH_PATH)\n",
    "\n",
    "\n",
    "for each in url_filtered_file_pairs:\n",
    "    filename = each[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = MIN_LENGTH_PATH + filename\n",
    "    print(output_file)\n",
    "\n",
    "    dF = dF[ dF['Text'].str.len() > MIN_LENGTH]\n",
    "    \n",
    "    # save\n",
    "    dF.to_csv(output_file, index=False, columns=['Text'])\n",
    "\n",
    "    print(dF, '\\n----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "- change case to smaller-case\n",
    "- remove punctuation\n",
    "- replace multiple consecutive spaces with single space\n",
    "- filter for MIN_LENGTH x 2\n",
    "- add class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:46.498077Z",
     "start_time": "2020-05-10T15:33:46.257358Z"
    }
   },
   "outputs": [],
   "source": [
    "file_pairs = get_file_pairs(MIN_LENGTH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:48.308742Z",
     "start_time": "2020-05-10T15:33:46.502177Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(PREPROCESSED_PATH):\n",
    "    os.makedirs(PREPROCESSED_PATH)\n",
    "\n",
    "for each in file_pairs:\n",
    "    file_name = each[0]\n",
    "    label = file_name.split('.csv')[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = PREPROCESSED_PATH + file_name\n",
    "\n",
    "    # convert to lower-case\n",
    "    dF.Text = dF.apply(lambda x: x.str.lower())\n",
    "    # remove punctuation\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(r'[^a-zA-Z_\\s0-9#]', ' '))\n",
    "    # remove extra spaces\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(r'\\s{2,}', ' '))\n",
    "    # filter for len(tweets) > MIN_LENGTH x 2 to get more meaningful tweets\n",
    "    dF = dF[ dF['Text'].str.len() > MIN_LENGTH * 2]\n",
    "\n",
    "    # add label\n",
    "    dF['Label'] = label\n",
    "\n",
    "    dF.to_csv(output_file, index=False, columns=['Label','Text'])\n",
    "    \n",
    "    print(dF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:48.445272Z",
     "start_time": "2020-05-10T15:33:48.310857Z"
    }
   },
   "outputs": [],
   "source": [
    "print('==========FINAL DATASET STATS==========\\n')\n",
    "for pair in get_prefixed_file_pairs(PREPROCESSED_PATH):\n",
    "    print('{0}\\t\\tLength: {1}'.format(pair[0], len(pair[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T08:17:55.125949Z",
     "start_time": "2020-05-09T08:17:55.121163Z"
    }
   },
   "source": [
    "### Create Dataset Files\n",
    "- single dataset file\n",
    "- split to train, test, validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:48.452917Z",
     "start_time": "2020-05-10T15:33:48.446787Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prefixed_file_pairs(files_dir):\n",
    "    files = glob.glob(files_dir + '*.csv')\n",
    "    file_pairs = []\n",
    "\n",
    "    for each in files:\n",
    "        file_name = each.split('/')[-1:][0]\n",
    "\n",
    "        dF = pd.read_csv(each, usecols=['Label', 'Text'])\n",
    "        dF['Text'] = dF['Text'].astype('str')\n",
    "        file_pairs.append((file_name, dF))\n",
    "        print('filename: {0}'.format(each))\n",
    "        print('size:     {0}'.format(dF.size))\n",
    "        print('----------------------------------------------------\\n')\n",
    "    \n",
    "    return file_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:48.604974Z",
     "start_time": "2020-05-10T15:33:48.454709Z"
    }
   },
   "outputs": [],
   "source": [
    "file_pairs = get_prefixed_file_pairs(PREPROCESSED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:48.919813Z",
     "start_time": "2020-05-10T15:33:48.606883Z"
    }
   },
   "outputs": [],
   "source": [
    "frames_list = []\n",
    "for each in file_pairs:\n",
    "    dF = each[1].copy()\n",
    "    frames_list.append(dF)\n",
    "\n",
    "bigDF = pd.concat(frames_list)\n",
    "\n",
    "if not os.path.exists(PROCESSED_PATH):\n",
    "    os.makedirs(PROCESSED_PATH)\n",
    "\n",
    "bigDF.to_csv(PROCESSED_PATH + 'dataset.csv', index=False, columns=['Label', 'Text'])\n",
    "\n",
    "print(bigDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Test, Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:49.044236Z",
     "start_time": "2020-05-10T15:33:48.921972Z"
    }
   },
   "outputs": [],
   "source": [
    "input_file = PROCESSED_PATH + 'dataset.csv';\n",
    "datasetFrame = pd.read_csv(input_file)\n",
    "print('dataset length:', len(datasetFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:49.080686Z",
     "start_time": "2020-05-10T15:33:49.047631Z"
    }
   },
   "outputs": [],
   "source": [
    "non_test_set, test_set = train_test_split(datasetFrame, test_size=0.2, random_state=18030010)\n",
    "train_set, val_set = train_test_split(non_test_set, test_size=0.15, random_state=18030010)\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T15:33:49.485384Z",
     "start_time": "2020-05-10T15:33:49.096497Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set.to_csv(PROCESSED_PATH + 'train.csv', index=False, columns=['Label', 'Text'])\n",
    "print('Train Set\\n',train_set)\n",
    "\n",
    "val_set.to_csv(PROCESSED_PATH + 'val.csv', index=False, columns=['Label', 'Text'])\n",
    "print('Val Set\\n',val_set)\n",
    "\n",
    "test_set.to_csv(PROCESSED_PATH + 'test.csv', index=False, columns=['Label', 'Text'])\n",
    "print('Test Set\\n',test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
