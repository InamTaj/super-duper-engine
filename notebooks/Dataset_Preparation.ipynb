{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:19.412595Z",
     "start_time": "2020-05-16T23:16:19.407554Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:19.430627Z",
     "start_time": "2020-05-16T23:16:19.421707Z"
    }
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "MIN_LENGTH = 40\n",
    "\n",
    "DATASET_PREFIX = '../dataset/'\n",
    "ENGLISH_PATH = DATASET_PREFIX + '1_english/'\n",
    "URLS_MENTIONS_REMOVED_PATH = DATASET_PREFIX + '2_filtered_urls_mentions/'\n",
    "MIN_LENGTH_PATH = DATASET_PREFIX + '3_length_greater_than_{0}/'.format(MIN_LENGTH)\n",
    "STOPWORDS_PATH = DATASET_PREFIX + '4_no-stopword/'\n",
    "NONROMAN_PATH = DATASET_PREFIX + '5_non-roman/'\n",
    "PREPROCESSED_PATH = DATASET_PREFIX + '6_pre-processed/'\n",
    "PROCESSED_PATH = DATASET_PREFIX + '7_processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:19.441064Z",
     "start_time": "2020-05-16T23:16:19.432497Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_file_pairs(files_dir):\n",
    "    files = glob.glob(files_dir + '*.csv')\n",
    "    file_pairs = []\n",
    "\n",
    "    for each in files:\n",
    "        file_name = each.split('/')[-1:][0]\n",
    "\n",
    "        dF = pd.read_csv(each, usecols=['Text'])\n",
    "        dF['Text'] = dF['Text'].astype('str')\n",
    "        file_pairs.append((file_name, dF))\n",
    "        print('filename: {0}'.format(each))\n",
    "        print('size:     {0}'.format(dF.size))\n",
    "        print('----------------------------------------------------\\n')\n",
    "    \n",
    "    return file_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Filter out URLs and @ Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:19.721805Z",
     "start_time": "2020-05-16T23:16:19.444766Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: ../dataset/1_english/narendramodi.csv\n",
      "size:     20173\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/1_english/mjibrannasir.csv\n",
      "size:     19275\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/1_english/maryamnsharif.csv\n",
      "size:     27681\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/1_english/sherryrehman.csv\n",
      "size:     18469\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/1_english/fawadchaudhry.csv\n",
      "size:     23525\n",
      "----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "english_file_pairs = get_file_pairs(ENGLISH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:21.374100Z",
     "start_time": "2020-05-16T23:16:19.723610Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Text\n",
      "0      Ramzan Mubarak! I pray for everyone’s safety, ...\n",
      "1      Best wishes to Goa CM   Ji on his birthday. Ma...\n",
      "2      Today’s discussion with Panchayat Sarpanchs wa...\n",
      "3      Interacting with Sarpanchs across the country ...\n",
      "4      Some of the steps taken to help the most vulne...\n",
      "...                                                  ...\n",
      "20168  Narendrabhai Modi: Through water conservation ...\n",
      "20169  Women would play a crucial role in the develop...\n",
      "20170  Most awaited gujarati version of www.narendram...\n",
      "20171  An inspiring address to Scouts & Guides in Jam...\n",
      "20172  2nd Feb, I will be in Dahod for Gujarat Swarni...\n",
      "\n",
      "[20173 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "                                                    Text\n",
      "0      When someone propagates and enforces ill infor...\n",
      "1      Tonight as our   and   teams deliver ration to...\n",
      "2      Alhumdulillah more than 5000+ families served ...\n",
      "3      So far whatever has been done in Pakistan to c...\n",
      "4      The Plan agreed between Govt and Ulema simply ...\n",
      "...                                                  ...\n",
      "19270  Omg! Omg! Omg! The Pepsi wali aunty is back wi...\n",
      "19271  We are going shopping tomorrow because Altaf b...\n",
      "19272  It was a small toast to life and possibilities...\n",
      "19273  I am sorry Ray Romano but in Pakistan, \"Everyb...\n",
      "19274  The only Pakistani leaders in (self) exile are...\n",
      "\n",
      "[19275 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "                                                    Text\n",
      "0      Mankind has not known a bigger sacrifice than ...\n",
      "1                                         #NewProfilePic\n",
      "2      Fervent prayers for his beloved nation from Mo...\n",
      "3      Heartfelt condolences to the families of those...\n",
      "4      Pakistan is facing one of the gravest crises i...\n",
      "...                                                  ...\n",
      "27676  Nawaz Sharif in Baluchistan today. Hope he suc...\n",
      "27677               Mariam Nawaz visits LAHORE College  \n",
      "27678     Maryam Nawaz - Future of pakistan's politics  \n",
      "27679               Maryam Nawaz Sharif Talks to Media  \n",
      "27680  Maryam Nawaz Sharif Talks to Media about her P...\n",
      "\n",
      "[27681 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "                                                    Text\n",
      "0                                    What a lovely story\n",
      "1             Lovely to see you two out there #diplomile\n",
      "2      And what about talking to Iran, which has a co...\n",
      "3      Criminal indeed to cut off communications duri...\n",
      "4                                                Whoa...\n",
      "...                                                  ...\n",
      "18464  Yes, the CJ should take suo moto notice of Sal...\n",
      "18465                                                   \n",
      "18466  Dear All, none of the FB accts in my name is m...\n",
      "18467  A special thank you to all those who have welc...\n",
      "18468  The battle for reclaiming Jinnah's Pakistan co...\n",
      "\n",
      "[18469 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "                                                    Text\n",
      "0      This shameless admission by frmr Indian Mily o...\n",
      "1      Thank you for standing up for truth #Kashmir #...\n",
      "2                                     Legend #AhmadFaraz\n",
      "3      Combating coronavirus: Made-in-Pakistan medica...\n",
      "4      Truly a remarkable decison by Cabinet to allow...\n",
      "...                                                  ...\n",
      "23520                           Gng back to lhr from isl\n",
      "23521          Gng for lunch somewhere in gulberg ll see\n",
      "23522     Just had a long discussion with president mush\n",
      "23523                                Lazy lamhay at home\n",
      "23524                      going to Royal palm for lunch\n",
      "\n",
      "[23525 rows x 1 columns] \n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# URLS_REMOVED_PATH\n",
    "regex_urls = r'(https?://.+\\b\\/?|.{3}\\.twitter.+\\b)'\n",
    "regex_mentions = r'@\\w+'\n",
    "regex = re.compile(regex_urls)\n",
    "\n",
    "if not os.path.exists(URLS_MENTIONS_REMOVED_PATH):\n",
    "    os.makedirs(URLS_MENTIONS_REMOVED_PATH)\n",
    "\n",
    "for each in english_file_pairs:\n",
    "    filename = each[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = URLS_MENTIONS_REMOVED_PATH + filename\n",
    "\n",
    "    # remove URLs\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(regex_urls, ' '))\n",
    "    # remove mentions\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(regex_mentions, ' '))\n",
    "    # save\n",
    "    dF.to_csv(output_file, index=False, columns=['Text'])\n",
    "    print(dF, '\\n----------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for MIN_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:21.652113Z",
     "start_time": "2020-05-16T23:16:21.376829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: ../dataset/2_filtered_urls_mentions/narendramodi.csv\n",
      "size:     20170\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/2_filtered_urls_mentions/mjibrannasir.csv\n",
      "size:     19206\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/2_filtered_urls_mentions/maryamnsharif.csv\n",
      "size:     27460\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/2_filtered_urls_mentions/sherryrehman.csv\n",
      "size:     18218\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/2_filtered_urls_mentions/fawadchaudhry.csv\n",
      "size:     23349\n",
      "----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url_filtered_file_pairs = get_file_pairs(URLS_MENTIONS_REMOVED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:22.253888Z",
     "start_time": "2020-05-16T23:16:21.654377Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dataset/3_length_greater_than_40/narendramodi.csv\n",
      "                                                    Text\n",
      "0      Ramzan Mubarak! I pray for everyone’s safety, ...\n",
      "1      Best wishes to Goa CM   Ji on his birthday. Ma...\n",
      "2      Today’s discussion with Panchayat Sarpanchs wa...\n",
      "3      Interacting with Sarpanchs across the country ...\n",
      "4      Some of the steps taken to help the most vulne...\n",
      "...                                                  ...\n",
      "20165  Narendrabhai Modi: Through water conservation ...\n",
      "20166  Women would play a crucial role in the develop...\n",
      "20167  Most awaited gujarati version of www.narendram...\n",
      "20168  An inspiring address to Scouts & Guides in Jam...\n",
      "20169  2nd Feb, I will be in Dahod for Gujarat Swarni...\n",
      "\n",
      "[19153 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "../dataset/3_length_greater_than_40/mjibrannasir.csv\n",
      "                                                    Text\n",
      "0      When someone propagates and enforces ill infor...\n",
      "1      Tonight as our   and   teams deliver ration to...\n",
      "2      Alhumdulillah more than 5000+ families served ...\n",
      "3      So far whatever has been done in Pakistan to c...\n",
      "4      The Plan agreed between Govt and Ulema simply ...\n",
      "...                                                  ...\n",
      "19201  Omg! Omg! Omg! The Pepsi wali aunty is back wi...\n",
      "19202  We are going shopping tomorrow because Altaf b...\n",
      "19203  It was a small toast to life and possibilities...\n",
      "19204  I am sorry Ray Romano but in Pakistan, \"Everyb...\n",
      "19205  The only Pakistani leaders in (self) exile are...\n",
      "\n",
      "[17010 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "../dataset/3_length_greater_than_40/maryamnsharif.csv\n",
      "                                                    Text\n",
      "0      Mankind has not known a bigger sacrifice than ...\n",
      "2      Fervent prayers for his beloved nation from Mo...\n",
      "3      Heartfelt condolences to the families of those...\n",
      "4      Pakistan is facing one of the gravest crises i...\n",
      "7      Another PML-N member arrested. This is their r...\n",
      "...                                                  ...\n",
      "27453  Nawaz calls for ending army operation in Baloc...\n",
      "27454      Nawaz urges urgent measures for Balochistan  \n",
      "27455  Nawaz Sharif in Baluchistan today. Hope he suc...\n",
      "27457     Maryam Nawaz - Future of pakistan's politics  \n",
      "27459  Maryam Nawaz Sharif Talks to Media about her P...\n",
      "\n",
      "[17578 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "../dataset/3_length_greater_than_40/sherryrehman.csv\n",
      "                                                    Text\n",
      "1             Lovely to see you two out there #diplomile\n",
      "2      And what about talking to Iran, which has a co...\n",
      "3      Criminal indeed to cut off communications duri...\n",
      "5      Please stay strong. They probably have limited...\n",
      "7      Horror stories on overseas Pakistanis left str...\n",
      "...                                                  ...\n",
      "18211  Our anchors should be hosting experts on joint...\n",
      "18214  Yes, the CJ should take suo moto notice of Sal...\n",
      "18215  Dear All, none of the FB accts in my name is m...\n",
      "18216  A special thank you to all those who have welc...\n",
      "18217  The battle for reclaiming Jinnah's Pakistan co...\n",
      "\n",
      "[14226 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "../dataset/3_length_greater_than_40/fawadchaudhry.csv\n",
      "                                                    Text\n",
      "0      This shameless admission by frmr Indian Mily o...\n",
      "1      Thank you for standing up for truth #Kashmir #...\n",
      "3      Combating coronavirus: Made-in-Pakistan medica...\n",
      "4      Truly a remarkable decison by Cabinet to allow...\n",
      "5      So grateful   endorsement from an expert like ...\n",
      "...                                                  ...\n",
      "23340  Jagday rehna saday tey na rehna (A public serv...\n",
      "23341  NEW CEO OF AL QAIDA...ANSAR ABBASI, IRFAN SADD...\n",
      "23342  USA-Pakistan lets do it OBL death is great suc...\n",
      "23345          Gng for lunch somewhere in gulberg ll see\n",
      "23346     Just had a long discussion with president mush\n",
      "\n",
      "[16934 rows x 1 columns] \n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MIN_LENGTH_PATH):\n",
    "    os.makedirs(MIN_LENGTH_PATH)\n",
    "\n",
    "\n",
    "for each in url_filtered_file_pairs:\n",
    "    filename = each[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = MIN_LENGTH_PATH + filename\n",
    "    print(output_file)\n",
    "\n",
    "    dF = dF[ dF['Text'].str.len() > MIN_LENGTH]\n",
    "    \n",
    "    # save\n",
    "    dF.to_csv(output_file, index=False, columns=['Text'])\n",
    "\n",
    "    print(dF, '\\n----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:32.861468Z",
     "start_time": "2020-05-16T23:16:22.256400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/inam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:33.125167Z",
     "start_time": "2020-05-16T23:16:32.865771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: ../dataset/3_length_greater_than_40/narendramodi.csv\n",
      "size:     19153\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/3_length_greater_than_40/mjibrannasir.csv\n",
      "size:     17010\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/3_length_greater_than_40/maryamnsharif.csv\n",
      "size:     17578\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/3_length_greater_than_40/sherryrehman.csv\n",
      "size:     14226\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/3_length_greater_than_40/fawadchaudhry.csv\n",
      "size:     16934\n",
      "----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_length_file_pairs = get_file_pairs(MIN_LENGTH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:41.044081Z",
     "start_time": "2020-05-16T23:16:33.137239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP_WORDS ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "output_file ../dataset/4_no-stopword/narendramodi.csv\n",
      "                                                    Text\n",
      "0      ramzan mubarak! pray everyone’s safety, well-b...\n",
      "1      best wishes goa cm   ji birthday. may blessed ...\n",
      "2      today’s discussion panchayat sarpanchs insight...\n",
      "3      interacting sarpanchs across country video-con...\n",
      "4                         steps taken help vulnerable...\n",
      "...                                                  ...\n",
      "19148  narendrabhai modi: water conservation movement...\n",
      "19149  women would play crucial role development guja...\n",
      "19150  awaited gujarati version www.narendramodi.in l...\n",
      "19151  inspiring address scouts & guides jamboree-200...\n",
      "19152       2nd feb, dahod gujarat swarnim jayanti yatra\n",
      "\n",
      "[19153 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "output_file ../dataset/4_no-stopword/mjibrannasir.csv\n",
      "                                                    Text\n",
      "0      someone propagates enforces ill informed polic...\n",
      "1      tonight     teams deliver ration 6000th family...\n",
      "2      alhumdulillah 5000+ families served     teams ...\n",
      "3      far whatever done pakistan control #corona goi...\n",
      "4      plan agreed govt ulema simply means put commun...\n",
      "...                                                  ...\n",
      "17005  omg! omg! omg! pepsi wali aunty back 5 rupee o...\n",
      "17006        going shopping tomorrow altaf bhai said so!\n",
      "17007       small toast life possibilities long overdue.\n",
      "17008  sorry ray romano pakistan, \"everybody hates ra...\n",
      "17009  pakistani leaders (self) exile pervez musharra...\n",
      "\n",
      "[17010 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "output_file ../dataset/4_no-stopword/maryamnsharif.csv\n",
      "                                                    Text\n",
      "0      mankind known bigger sacrifice save lives impe...\n",
      "1      fervent prayers beloved nation mohammad nawaz ...\n",
      "2      heartfelt condolences families succumbed epide...\n",
      "3      pakistan facing one gravest crises recent memo...\n",
      "4      another pml-n member arrested. response histor...\n",
      "...                                                  ...\n",
      "17573  nawaz calls ending army operation balochistan ...\n",
      "17574          nawaz urges urgent measures balochistan  \n",
      "17575  nawaz sharif baluchistan today. hope succeeds ...\n",
      "17576        maryam nawaz - future pakistan's politics  \n",
      "17577  maryam nawaz sharif talks media political ambi...\n",
      "\n",
      "[17578 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "output_file ../dataset/4_no-stopword/sherryrehman.csv\n",
      "                                                    Text\n",
      "0                              lovely see two #diplomile\n",
      "1      talking iran, contiguous border substantial le...\n",
      "2      criminal indeed cut communications outbreak. d...\n",
      "3      please stay strong. probably limited capacity,...\n",
      "4      horror stories overseas pakistanis left strand...\n",
      "...                                                  ...\n",
      "14221  anchors hosting experts joint indopk managemen...\n",
      "14222  yes, cj take suo moto notice salman taseer's m...\n",
      "14223  dear all, none fb accts name mine. written fb ...\n",
      "14224  special thank welcomed warmly twitter. berry h...\n",
      "14225  battle reclaiming jinnah's pakistan continues....\n",
      "\n",
      "[14226 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "output_file ../dataset/4_no-stopword/fawadchaudhry.csv\n",
      "                                                    Text\n",
      "0      shameless admission frmr indian mily officer c...\n",
      "1           thank standing truth #kashmir #kashmirbleeds\n",
      "2      combating coronavirus: made-in-pakistan medica...\n",
      "3      truly remarkable decison cabinet allow export ...\n",
      "4      grateful   endorsement expert like means lot s...\n",
      "...                                                  ...\n",
      "16929  jagday rehna saday tey na rehna (a public serv...\n",
      "16930  new ceo al qaida...ansar abbasi, irfan saddiqu...\n",
      "16931  usa-pakistan lets obl death great success fini...\n",
      "16932                    gng lunch somewhere gulberg see\n",
      "16933                     long discussion president mush\n",
      "\n",
      "[16934 rows x 1 columns] \n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(STOPWORDS_PATH):\n",
    "    os.makedirs(STOPWORDS_PATH)\n",
    "\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "print('STOP_WORDS', STOP_WORDS)\n",
    "    \n",
    "def remove_stopwords(texts):\n",
    "    new_texts = []\n",
    "    for tx in texts:\n",
    "        splitted = tx.lower().split(' ')\n",
    "        removed = [word for word in splitted if not word in STOP_WORDS]\n",
    "        removed = ' '.join(removed)\n",
    "        new_texts.append(removed)\n",
    "    return new_texts\n",
    "\n",
    "for each in min_length_file_pairs:\n",
    "    filename = each[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = STOPWORDS_PATH + filename\n",
    "    print('output_file', output_file)\n",
    "\n",
    "    # remove all stop_words\n",
    "    dF.Text = remove_stopwords(dF.Text)\n",
    "\n",
    "    # save\n",
    "    dF.to_csv(output_file, index=False, columns=['Text'])\n",
    "\n",
    "    print(dF, '\\n----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roman English Removal\n",
    "> Removes roman english tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:41.052299Z",
     "start_time": "2020-05-16T23:16:41.046569Z"
    }
   },
   "outputs": [],
   "source": [
    "from enchant.checker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:16:41.249271Z",
     "start_time": "2020-05-16T23:16:41.055260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: ../dataset/4_no-stopword/narendramodi.csv\n",
      "size:     19153\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/4_no-stopword/mjibrannasir.csv\n",
      "size:     17010\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/4_no-stopword/maryamnsharif.csv\n",
      "size:     17578\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/4_no-stopword/sherryrehman.csv\n",
      "size:     14226\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/4_no-stopword/fawadchaudhry.csv\n",
      "size:     16934\n",
      "----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopwords_file_pairs = get_file_pairs(STOPWORDS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:17:16.618080Z",
     "start_time": "2020-05-16T23:16:41.251298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_file ../dataset/5_non-roman/narendramodi.csv\n",
      "                                                    Text\n",
      "0      ramzan mubarak! pray everyone’s safety, well-b...\n",
      "1      best wishes goa cm   ji birthday. may blessed ...\n",
      "2      today’s discussion panchayat sarpanchs insight...\n",
      "3      interacting sarpanchs across country video-con...\n",
      "4                         steps taken help vulnerable...\n",
      "...                                                  ...\n",
      "18521  reading: \"modi dedicates cybre long distance e...\n",
      "18522  narendrabhai modi: water conservation movement...\n",
      "18523  women would play crucial role development guja...\n",
      "18524  awaited gujarati version www.narendramodi.in l...\n",
      "18525  inspiring address scouts & guides jamboree-200...\n",
      "\n",
      "[18526 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "output_file ../dataset/5_non-roman/mjibrannasir.csv\n",
      "                                                    Text\n",
      "0      someone propagates enforces ill informed polic...\n",
      "1      tonight     teams deliver ration 6000th family...\n",
      "2      alhumdulillah 5000+ families served     teams ...\n",
      "3      far whatever done pakistan control #corona goi...\n",
      "4      plan agreed govt ulema simply means put commun...\n",
      "...                                                  ...\n",
      "14975  custom mix special sheesha, waheed's kabab fry...\n",
      "14976  everything - something : life. looking something.\n",
      "14977        going shopping tomorrow altaf bhai said so!\n",
      "14978       small toast life possibilities long overdue.\n",
      "14979  sorry ray romano pakistan, \"everybody hates ra...\n",
      "\n",
      "[14980 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "output_file ../dataset/5_non-roman/maryamnsharif.csv\n",
      "                                                    Text\n",
      "0      mankind known bigger sacrifice save lives impe...\n",
      "1      fervent prayers beloved nation mohammad nawaz ...\n",
      "2      heartfelt condolences families succumbed epide...\n",
      "3      pakistan facing one gravest crises recent memo...\n",
      "4      another pml-n member arrested. response histor...\n",
      "...                                                  ...\n",
      "16080  nawaz calls ending army operation balochistan ...\n",
      "16081          nawaz urges urgent measures balochistan  \n",
      "16082  nawaz sharif baluchistan today. hope succeeds ...\n",
      "16083        maryam nawaz - future pakistan's politics  \n",
      "16084  maryam nawaz sharif talks media political ambi...\n",
      "\n",
      "[16085 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "output_file ../dataset/5_non-roman/sherryrehman.csv\n",
      "                                                    Text\n",
      "0                              lovely see two #diplomile\n",
      "1      talking iran, contiguous border substantial le...\n",
      "2      criminal indeed cut communications outbreak. d...\n",
      "3      please stay strong. probably limited capacity,...\n",
      "4      horror stories overseas pakistanis left strand...\n",
      "...                                                  ...\n",
      "13883  anchors hosting experts joint indopk managemen...\n",
      "13884  yes, cj take suo moto notice salman taseer's m...\n",
      "13885  dear all, none fb accts name mine. written fb ...\n",
      "13886  special thank welcomed warmly twitter. berry h...\n",
      "13887  battle reclaiming jinnah's pakistan continues....\n",
      "\n",
      "[13888 rows x 1 columns] \n",
      "----------------------------------------------------\n",
      "output_file ../dataset/5_non-roman/fawadchaudhry.csv\n",
      "                                                    Text\n",
      "0      shameless admission frmr indian mily officer c...\n",
      "1           thank standing truth #kashmir #kashmirbleeds\n",
      "2      combating coronavirus: made-in-pakistan medica...\n",
      "3      truly remarkable decison cabinet allow export ...\n",
      "4      grateful   endorsement expert like means lot s...\n",
      "...                                                  ...\n",
      "12446  heat dubai thinking whats biiger violation fac...\n",
      "12447  seems favorite pass game islamabad blaming mus...\n",
      "12448  usa-pakistan lets obl death great success fini...\n",
      "12449                    gng lunch somewhere gulberg see\n",
      "12450                     long discussion president mush\n",
      "\n",
      "[12451 rows x 1 columns] \n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(NONROMAN_PATH):\n",
    "    os.makedirs(NONROMAN_PATH)\n",
    "\n",
    "\n",
    "ROMAN_THRESHOLD = .5\n",
    "\n",
    "def remove_romans(texts):\n",
    "    \"\"\"\n",
    "    Check if 50% of each sentence has valid english words\n",
    "    \"\"\"\n",
    "    new_texts = []\n",
    "    d = SpellChecker(\"en_US\")\n",
    "    \n",
    "    for quote in texts:      \n",
    "        d.set_text(quote)\n",
    "        errors = [err.word for err in d]\n",
    "        target_length = len(quote.split()) * ROMAN_THRESHOLD\n",
    "        if len(errors) <= target_length:\n",
    "            new_texts.append(quote)\n",
    "\n",
    "    return pd.DataFrame({'Text': new_texts})\n",
    "\n",
    "for each in stopwords_file_pairs:\n",
    "    filename = each[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = NONROMAN_PATH + filename\n",
    "    print('output_file', output_file)\n",
    "\n",
    "    # filter sentences that have more than 50% non-english content\n",
    "    dF = remove_romans(dF.Text)\n",
    "\n",
    "    # save\n",
    "    dF.to_csv(output_file, index=False, columns=['Text'])\n",
    "\n",
    "    print(dF, '\\n----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "- change case to smaller-case\n",
    "- remove punctuation\n",
    "- replace multiple consecutive spaces with single space\n",
    "- filter for MIN_LENGTH x 2\n",
    "- add class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:17:16.758521Z",
     "start_time": "2020-05-16T23:17:16.619793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: ../dataset/5_non-roman/narendramodi.csv\n",
      "size:     18526\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/5_non-roman/mjibrannasir.csv\n",
      "size:     14980\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/5_non-roman/maryamnsharif.csv\n",
      "size:     16085\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/5_non-roman/sherryrehman.csv\n",
      "size:     13888\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/5_non-roman/fawadchaudhry.csv\n",
      "size:     12451\n",
      "----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_pairs = get_file_pairs(NONROMAN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:17:18.556779Z",
     "start_time": "2020-05-16T23:17:16.760540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Text         Label\n",
      "0      ramzan mubarak pray everyones safety wellbeing...  narendramodi\n",
      "1      best wishes goa cm ji birthday may blessed lon...  narendramodi\n",
      "2      todays discussion panchayat sarpanchs insightf...  narendramodi\n",
      "3      interacting sarpanchs across country videoconf...  narendramodi\n",
      "5      exchanged views covid19 pandemic pm thanked su...  narendramodi\n",
      "...                                                  ...           ...\n",
      "18521  reading modi dedicates cybre long distance edu...  narendramodi\n",
      "18522  narendrabhai modi water conservation movement ...  narendramodi\n",
      "18523  women would play crucial role development guja...  narendramodi\n",
      "18524  awaited gujarati version wwwnarendramodiin lau...  narendramodi\n",
      "18525  inspiring address scouts guides jamboree2009 w...  narendramodi\n",
      "\n",
      "[15686 rows x 2 columns]\n",
      "                                                    Text         Label\n",
      "0      someone propagates enforces ill informed polic...  mjibrannasir\n",
      "1      tonight teams deliver ration 6000th family tha...  mjibrannasir\n",
      "2      alhumdulillah 5000 families served teams 1 mon...  mjibrannasir\n",
      "3      far whatever done pakistan control #corona goi...  mjibrannasir\n",
      "4      plan agreed govt ulema simply means put commun...  mjibrannasir\n",
      "...                                                  ...           ...\n",
      "14970  sachin scores 100 centuries become mp miandad ...  mjibrannasir\n",
      "14971  #lyari consumes sho #gillani busy making victo...  mjibrannasir\n",
      "14972  post midnight saw homeless kids sleeping next ...  mjibrannasir\n",
      "14973  imran khan u sit every time nation calling u s...  mjibrannasir\n",
      "14975  custom mix special sheesha waheeds kabab fry s...  mjibrannasir\n",
      "\n",
      "[11334 rows x 2 columns]\n",
      "                                                    Text          Label\n",
      "0      mankind known bigger sacrifice save lives impe...  maryamnsharif\n",
      "1      fervent prayers beloved nation mohammad nawaz ...  maryamnsharif\n",
      "2      heartfelt condolences families succumbed epide...  maryamnsharif\n",
      "3      pakistan facing one gravest crises recent memo...  maryamnsharif\n",
      "4      another pmln member arrested response historic...  maryamnsharif\n",
      "...                                                  ...            ...\n",
      "16073   mind all however ur question valid amp releva...  maryamnsharif\n",
      "16076  hurling blames others remember nothing u utter...  maryamnsharif\n",
      "16077   ashamed tainted father figure like nawaz shar...  maryamnsharif\n",
      "16078  zardari sb u parried pertinent burning questio...  maryamnsharif\n",
      "16082  nawaz sharif baluchistan today hope succeeds r...  maryamnsharif\n",
      "\n",
      "[8814 rows x 2 columns]\n",
      "                                                    Text         Label\n",
      "1      talking iran contiguous border substantial lev...  sherryrehman\n",
      "2      criminal indeed cut communications outbreak di...  sherryrehman\n",
      "3      please stay strong probably limited capacity d...  sherryrehman\n",
      "4      horror stories overseas pakistanis left strand...  sherryrehman\n",
      "6      covid19 reshape afghan peace conventional wisd...  sherryrehman\n",
      "...                                                  ...           ...\n",
      "13883  anchors hosting experts joint indopk managemen...  sherryrehman\n",
      "13884  yes cj take suo moto notice salman taseers mur...  sherryrehman\n",
      "13885  dear all none fb accts name mine written fb do...  sherryrehman\n",
      "13886  special thank welcomed warmly twitter berry ho...  sherryrehman\n",
      "13887  battle reclaiming jinnahs pakistan continues s...  sherryrehman\n",
      "\n",
      "[10046 rows x 2 columns]\n",
      "                                                    Text          Label\n",
      "0      shameless admission frmr indian mily officer c...  fawadchaudhry\n",
      "2      combating coronavirus madeinpakistan medical e...  fawadchaudhry\n",
      "3      truly remarkable decison cabinet allow export ...  fawadchaudhry\n",
      "4      grateful endorsement expert like means lot spe...  fawadchaudhry\n",
      "5      thankyou hosting event free sending masks sani...  fawadchaudhry\n",
      "...                                                  ...            ...\n",
      "12441   agree brings in democrats isnt democracy sans...  fawadchaudhry\n",
      "12444  naval base attackeddont remember last good new...  fawadchaudhry\n",
      "12445  establishments policy inflaming hatred usa may...  fawadchaudhry\n",
      "12446  heat dubai thinking whats biiger violation fac...  fawadchaudhry\n",
      "12447  seems favorite pass game islamabad blaming mus...  fawadchaudhry\n",
      "\n",
      "[7984 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(PREPROCESSED_PATH):\n",
    "    os.makedirs(PREPROCESSED_PATH)\n",
    "\n",
    "for each in file_pairs:\n",
    "    file_name = each[0]\n",
    "    label = file_name.split('.csv')[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = PREPROCESSED_PATH + file_name\n",
    "\n",
    "    # convert to lower-case\n",
    "    dF.Text = dF.apply(lambda x: x.str.lower())\n",
    "    # remove punctuation\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(r\"[^a-zA-Z_\\s0-9#]\", ''))\n",
    "    # remove extra spaces\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(r'\\s{2,}', ' '))\n",
    "    # filter for len(tweets) > MIN_LENGTH x 2 to get more meaningful tweets\n",
    "    dF = dF[ dF['Text'].str.len() > MIN_LENGTH * 1.5]\n",
    "\n",
    "    # add label\n",
    "    dF['Label'] = label\n",
    "\n",
    "    dF.to_csv(output_file, index=False, columns=['Label','Text'])\n",
    "    \n",
    "    print(dF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:17:18.658181Z",
     "start_time": "2020-05-16T23:17:18.558320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========FINAL DATASET STATS==========\n",
      "\n",
      "filename: ../dataset/6_pre-processed/narendramodi.csv\n",
      "size:     15686\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/6_pre-processed/mjibrannasir.csv\n",
      "size:     11334\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/6_pre-processed/maryamnsharif.csv\n",
      "size:     8814\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/6_pre-processed/sherryrehman.csv\n",
      "size:     10046\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/6_pre-processed/fawadchaudhry.csv\n",
      "size:     7984\n",
      "----------------------------------------------------\n",
      "\n",
      "narendramodi.csv\t\tLength: 15686\n",
      "mjibrannasir.csv\t\tLength: 11334\n",
      "maryamnsharif.csv\t\tLength: 8814\n",
      "sherryrehman.csv\t\tLength: 10046\n",
      "fawadchaudhry.csv\t\tLength: 7984\n"
     ]
    }
   ],
   "source": [
    "print('==========FINAL DATASET STATS==========\\n')\n",
    "for pair in get_file_pairs(PREPROCESSED_PATH):\n",
    "    print('{0}\\t\\tLength: {1}'.format(pair[0], len(pair[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T08:17:55.125949Z",
     "start_time": "2020-05-09T08:17:55.121163Z"
    }
   },
   "source": [
    "### Create Dataset Files\n",
    "- single dataset file\n",
    "- split to train, test, validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:17:18.666890Z",
     "start_time": "2020-05-16T23:17:18.659602Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prefixed_file_pairs(files_dir):\n",
    "    files = glob.glob(files_dir + '*.csv')\n",
    "    file_pairs = []\n",
    "\n",
    "    for each in files:\n",
    "        file_name = each.split('/')[-1:][0]\n",
    "\n",
    "        dF = pd.read_csv(each, usecols=['Label', 'Text'])\n",
    "        dF['Text'] = dF['Text'].astype('str')\n",
    "        file_pairs.append((file_name, dF))\n",
    "        print('filename: {0}'.format(each))\n",
    "        print('size:     {0}'.format(len(dF)))\n",
    "        print('----------------------------------------------------\\n')\n",
    "    \n",
    "    return file_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:17:18.754823Z",
     "start_time": "2020-05-16T23:17:18.668992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: ../dataset/6_pre-processed/narendramodi.csv\n",
      "size:     15686\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/6_pre-processed/mjibrannasir.csv\n",
      "size:     11334\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/6_pre-processed/maryamnsharif.csv\n",
      "size:     8814\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/6_pre-processed/sherryrehman.csv\n",
      "size:     10046\n",
      "----------------------------------------------------\n",
      "\n",
      "filename: ../dataset/6_pre-processed/fawadchaudhry.csv\n",
      "size:     7984\n",
      "----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_pairs = get_prefixed_file_pairs(PREPROCESSED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:17:19.038685Z",
     "start_time": "2020-05-16T23:17:18.756381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Label                                               Text\n",
      "0      narendramodi  ramzan mubarak pray everyones safety wellbeing...\n",
      "1      narendramodi  best wishes goa cm ji birthday may blessed lon...\n",
      "2      narendramodi  todays discussion panchayat sarpanchs insightf...\n",
      "3      narendramodi  interacting sarpanchs across country videoconf...\n",
      "4      narendramodi  exchanged views covid19 pandemic pm thanked su...\n",
      "...             ...                                                ...\n",
      "7979  fawadchaudhry   agree brings in democrats isnt democracy sans...\n",
      "7980  fawadchaudhry  naval base attackeddont remember last good new...\n",
      "7981  fawadchaudhry  establishments policy inflaming hatred usa may...\n",
      "7982  fawadchaudhry  heat dubai thinking whats biiger violation fac...\n",
      "7983  fawadchaudhry  seems favorite pass game islamabad blaming mus...\n",
      "\n",
      "[53864 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "frames_list = []\n",
    "for each in file_pairs:\n",
    "    dF = each[1].copy()\n",
    "    frames_list.append(dF)\n",
    "\n",
    "bigDF = pd.concat(frames_list)\n",
    "\n",
    "if not os.path.exists(PROCESSED_PATH):\n",
    "    os.makedirs(PROCESSED_PATH)\n",
    "\n",
    "bigDF.to_csv(PROCESSED_PATH + 'dataset.csv', index=False, columns=['Label', 'Text'])\n",
    "\n",
    "print(bigDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Test, Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:17:19.136883Z",
     "start_time": "2020-05-16T23:17:19.041374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: 53864\n"
     ]
    }
   ],
   "source": [
    "input_file = PROCESSED_PATH + 'dataset.csv';\n",
    "datasetFrame = pd.read_csv(input_file)\n",
    "print('dataset length:', len(datasetFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:17:19.169336Z",
     "start_time": "2020-05-16T23:17:19.138919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36627, 6464, 10773)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_test_set, test_set = train_test_split(datasetFrame, test_size=0.2, random_state=18030010)\n",
    "train_set, val_set = train_test_split(non_test_set, test_size=0.15, random_state=18030010)\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T23:17:19.507958Z",
     "start_time": "2020-05-16T23:17:19.171790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "                Label                                               Text\n",
      "30193  maryamnsharif  remove dark shades see historic jalsi jalsa pl...\n",
      "17762   mjibrannasir  several issues connect us citizens growing can...\n",
      "24841   mjibrannasir  taking effort forward sindh govt make prominen...\n",
      "18156   mjibrannasir  2018 commonwealth games erratic india punished...\n",
      "24583   mjibrannasir   people islamabad tell differently people g6 s...\n",
      "...              ...                                                ...\n",
      "40204   sherryrehman  na committee allowed gag cybercrime bill go th...\n",
      "35301  maryamnsharif   unbecoming attitude warrants response v civil...\n",
      "11686   narendramodi  thoughts prayers families lost lives board fli...\n",
      "46868  fawadchaudhry  hi allah g ik said something bc tht ne er said...\n",
      "3246    narendramodi  ekta bhyan started playing sports regular basi...\n",
      "\n",
      "[36627 rows x 2 columns]\n",
      "Val Set\n",
      "                Label                                               Text\n",
      "35943   sherryrehman  without enough test kits face terrible crisis ...\n",
      "40171   sherryrehman  ability see ones society group critical eye su...\n",
      "48903  fawadchaudhry  dear gen govt says army intervention nt reques...\n",
      "49646  fawadchaudhry  welcome step cjp frmr cjp ifti ch business par...\n",
      "43863   sherryrehman   proud work u do really changing lives bottom ...\n",
      "...              ...                                                ...\n",
      "16665   mjibrannasir  pir uzair shah arrested #maria case update mar...\n",
      "10730   narendramodi  martyrs day pay homage every martyr laid life ...\n",
      "46613  fawadchaudhry  ppp politicised #benazirbhutto case tht result...\n",
      "36686   sherryrehman  pakistan team every pakistani rooting today 15...\n",
      "46944  fawadchaudhry  times muslim countries dream choose govt pak e...\n",
      "\n",
      "[6464 rows x 2 columns]\n",
      "Test Set\n",
      "                Label                                               Text\n",
      "29500  maryamnsharif  meeting pms youth business loans chaired pm ns...\n",
      "579     narendramodi  among many success stories related governance ...\n",
      "21842   mjibrannasir  fact u keep saying had faith says easy waiver ...\n",
      "32653  maryamnsharif  thanks honour honor following u twitter dear r...\n",
      "48164  fawadchaudhry   two nation theory came end creation bd also j...\n",
      "...              ...                                                ...\n",
      "12421   narendramodi  rich congress leaders politics hobby us medium...\n",
      "11179   narendramodi  legal luminary philosopher intellectual justic...\n",
      "6763    narendramodi  national health policy marks historic moment e...\n",
      "40429   sherryrehman  yes #climatechange factor nothing happened una...\n",
      "49181  fawadchaudhry  survive one virgintake 71 back lieu mouthful f...\n",
      "\n",
      "[10773 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_set.to_csv(PROCESSED_PATH + 'train.csv', index=False, columns=['Label', 'Text'])\n",
    "print('Train Set\\n',train_set)\n",
    "\n",
    "val_set.to_csv(PROCESSED_PATH + 'val.csv', index=False, columns=['Label', 'Text'])\n",
    "print('Val Set\\n',val_set)\n",
    "\n",
    "test_set.to_csv(PROCESSED_PATH + 'test.csv', index=False, columns=['Label', 'Text'])\n",
    "print('Test Set\\n',test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
