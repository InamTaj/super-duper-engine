{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:07.563901Z",
     "start_time": "2020-05-17T20:59:07.557618Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:07.574169Z",
     "start_time": "2020-05-17T20:59:07.567160Z"
    }
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "MIN_LENGTH = 80\n",
    "\n",
    "DATASET_PREFIX = '../dataset/'\n",
    "ENGLISH_PATH = DATASET_PREFIX + '1_english/'\n",
    "URLS_MENTIONS_REMOVED_PATH = DATASET_PREFIX + '2_filtered_urls_mentions/'\n",
    "MIN_LENGTH_PATH = DATASET_PREFIX + '3_length_greater_than_{0}/'.format(MIN_LENGTH)\n",
    "STOPWORDS_PATH = DATASET_PREFIX + '4_no-stopword/'\n",
    "NONROMAN_PATH = DATASET_PREFIX + '5_non-roman/'\n",
    "PREPROCESSED_PATH = DATASET_PREFIX + '6_pre-processed/'\n",
    "PROCESSED_PATH = DATASET_PREFIX + '7_processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:07.585191Z",
     "start_time": "2020-05-17T20:59:07.576865Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_file_pairs(files_dir):\n",
    "    files = glob.glob(files_dir + '*.csv')\n",
    "    file_pairs = []\n",
    "\n",
    "    for each in files:\n",
    "        file_name = each.split('/')[-1:][0]\n",
    "\n",
    "        dF = pd.read_csv(each, usecols=['Text'])\n",
    "        dF['Text'] = dF['Text'].astype('str')\n",
    "        file_pairs.append((file_name, dF))\n",
    "        print('filename: {0}'.format(each))\n",
    "        print('size:     {0}'.format(dF.size))\n",
    "        print('----------------------------------------------------\\n')\n",
    "    \n",
    "    return file_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out URLs and @ Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:07.878892Z",
     "start_time": "2020-05-17T20:59:07.588079Z"
    }
   },
   "outputs": [],
   "source": [
    "english_file_pairs = get_file_pairs(ENGLISH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:09.405689Z",
     "start_time": "2020-05-17T20:59:07.880819Z"
    }
   },
   "outputs": [],
   "source": [
    "# URLS_REMOVED_PATH\n",
    "regex_urls = r'(https?://.+\\b\\/?|.{3}\\.twitter.+\\b)'\n",
    "regex_mentions = r'@\\w+'\n",
    "regex = re.compile(regex_urls)\n",
    "\n",
    "if not os.path.exists(URLS_MENTIONS_REMOVED_PATH):\n",
    "    os.makedirs(URLS_MENTIONS_REMOVED_PATH)\n",
    "\n",
    "for each in english_file_pairs:\n",
    "    filename = each[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = URLS_MENTIONS_REMOVED_PATH + filename\n",
    "\n",
    "    # remove URLs\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(regex_urls, ' '))\n",
    "    # remove mentions\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(regex_mentions, ' '))\n",
    "    # save\n",
    "    dF.to_csv(output_file, index=False, columns=['Text'])\n",
    "    print(dF, '\\n----------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for MIN_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:09.599813Z",
     "start_time": "2020-05-17T20:59:09.408445Z"
    }
   },
   "outputs": [],
   "source": [
    "url_filtered_file_pairs = get_file_pairs(URLS_MENTIONS_REMOVED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:10.129221Z",
     "start_time": "2020-05-17T20:59:09.601464Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(MIN_LENGTH_PATH):\n",
    "    os.makedirs(MIN_LENGTH_PATH)\n",
    "\n",
    "\n",
    "for each in url_filtered_file_pairs:\n",
    "    filename = each[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = MIN_LENGTH_PATH + filename\n",
    "    print(output_file)\n",
    "\n",
    "    dF = dF[ dF['Text'].str.len() > MIN_LENGTH]\n",
    "    \n",
    "    # save\n",
    "    dF.to_csv(output_file, index=False, columns=['Text'])\n",
    "\n",
    "    print(dF, '\\n----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:10.137969Z",
     "start_time": "2020-05-17T20:59:10.132543Z"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:10.143593Z",
     "start_time": "2020-05-17T20:59:10.140855Z"
    }
   },
   "outputs": [],
   "source": [
    "# min_length_file_pairs = get_file_pairs(MIN_LENGTH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:10.149674Z",
     "start_time": "2020-05-17T20:59:10.146009Z"
    }
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists(STOPWORDS_PATH):\n",
    "#     os.makedirs(STOPWORDS_PATH)\n",
    "\n",
    "# STOP_WORDS = stopwords.words('english')\n",
    "# print('STOP_WORDS', STOP_WORDS)\n",
    "    \n",
    "# def remove_stopwords(texts):\n",
    "#     new_texts = []\n",
    "#     for tx in texts:\n",
    "#         splitted = tx.lower().split(' ')\n",
    "#         removed = [word for word in splitted if not word in STOP_WORDS]\n",
    "#         removed = ' '.join(removed)\n",
    "#         new_texts.append(removed)\n",
    "#     return new_texts\n",
    "\n",
    "# for each in min_length_file_pairs:\n",
    "#     filename = each[0]\n",
    "#     dF = each[1].copy()\n",
    "#     output_file = STOPWORDS_PATH + filename\n",
    "#     print('output_file', output_file)\n",
    "\n",
    "#     # remove all stop_words\n",
    "#     dF.Text = remove_stopwords(dF.Text)\n",
    "\n",
    "#     # save\n",
    "#     dF.to_csv(output_file, index=False, columns=['Text'])\n",
    "\n",
    "#     print(dF, '\\n----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roman English Removal\n",
    "> Removes roman english tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:10.154494Z",
     "start_time": "2020-05-17T20:59:10.151444Z"
    }
   },
   "outputs": [],
   "source": [
    "from enchant.checker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:10.299685Z",
     "start_time": "2020-05-17T20:59:10.157900Z"
    }
   },
   "outputs": [],
   "source": [
    "minlength_file_pairs = get_file_pairs(MIN_LENGTH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:52.284875Z",
     "start_time": "2020-05-17T20:59:10.301390Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(NONROMAN_PATH):\n",
    "    os.makedirs(NONROMAN_PATH)\n",
    "\n",
    "\n",
    "ROMAN_THRESHOLD = .5\n",
    "\n",
    "def remove_romans(texts):\n",
    "    \"\"\"\n",
    "    Check if 50% of each sentence has valid english words\n",
    "    \"\"\"\n",
    "    new_texts = []\n",
    "    d = SpellChecker(\"en_US\")\n",
    "    \n",
    "    for quote in texts:      \n",
    "        d.set_text(quote)\n",
    "        errors = [err.word for err in d]\n",
    "        target_length = len(quote.split()) * ROMAN_THRESHOLD\n",
    "        if len(errors) <= target_length:\n",
    "            new_texts.append(quote)\n",
    "\n",
    "    return pd.DataFrame({'Text': new_texts})\n",
    "\n",
    "for each in minlength_file_pairs:\n",
    "    filename = each[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = NONROMAN_PATH + filename\n",
    "    print('output_file', output_file)\n",
    "\n",
    "    # filter sentences that have more than 50% non-english content\n",
    "    dF = remove_romans(dF.Text)\n",
    "\n",
    "    # save\n",
    "    dF.to_csv(output_file, index=False, columns=['Text'])\n",
    "\n",
    "    print(dF, '\\n----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "- change case to smaller-case\n",
    "- remove punctuation\n",
    "- replace multiple consecutive spaces with single space\n",
    "- filter for MIN_LENGTH\n",
    "- add class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:52.418942Z",
     "start_time": "2020-05-17T20:59:52.286680Z"
    }
   },
   "outputs": [],
   "source": [
    "file_pairs = get_file_pairs(NONROMAN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:53.952142Z",
     "start_time": "2020-05-17T20:59:52.422158Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(PREPROCESSED_PATH):\n",
    "    os.makedirs(PREPROCESSED_PATH)\n",
    "\n",
    "for each in file_pairs:\n",
    "    file_name = each[0]\n",
    "    label = file_name.split('.csv')[0]\n",
    "    dF = each[1].copy()\n",
    "    output_file = PREPROCESSED_PATH + file_name\n",
    "\n",
    "    # convert to lower-case\n",
    "    dF.Text = dF.apply(lambda x: x.str.lower())\n",
    "    # remove punctuation\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(r\"[^a-zA-Z_\\s0-9#]\", ''))\n",
    "    # remove extra spaces\n",
    "    dF.Text = dF.apply(lambda x: x.str.replace(r'\\s{2,}', ' '))\n",
    "    # filter for len(tweets) > MIN_LENGTH x 2 to get more meaningful tweets\n",
    "    dF = dF[ dF['Text'].str.len() > MIN_LENGTH]\n",
    "\n",
    "    # add label\n",
    "    dF['Label'] = label\n",
    "\n",
    "    dF.to_csv(output_file, index=False, columns=['Label','Text'])\n",
    "    \n",
    "    print(dF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T08:17:55.125949Z",
     "start_time": "2020-05-09T08:17:55.121163Z"
    }
   },
   "source": [
    "### Create Dataset Files\n",
    "- single dataset file\n",
    "- split to train, test, validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:53.968327Z",
     "start_time": "2020-05-17T20:59:53.959226Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prefixed_file_pairs(files_dir):\n",
    "    files = glob.glob(files_dir + '*.csv')\n",
    "    file_pairs = []\n",
    "\n",
    "    for each in files:\n",
    "        file_name = each.split('/')[-1:][0]\n",
    "\n",
    "        dF = pd.read_csv(each, usecols=['Label', 'Text'])\n",
    "        dF['Text'] = dF['Text'].astype('str')\n",
    "        file_pairs.append((file_name, dF))\n",
    "        print('filename: {0}'.format(each))\n",
    "        print('size:     {0}'.format(len(dF)))\n",
    "        print('----------------------------------------------------\\n')\n",
    "    \n",
    "    return file_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:54.172341Z",
     "start_time": "2020-05-17T20:59:53.977210Z"
    }
   },
   "outputs": [],
   "source": [
    "file_pairs = get_prefixed_file_pairs(PREPROCESSED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:54.476198Z",
     "start_time": "2020-05-17T20:59:54.174218Z"
    }
   },
   "outputs": [],
   "source": [
    "frames_list = []\n",
    "for each in file_pairs:\n",
    "    dF = each[1].copy()\n",
    "    frames_list.append(dF)\n",
    "\n",
    "bigDF = pd.concat(frames_list)\n",
    "\n",
    "if not os.path.exists(PROCESSED_PATH):\n",
    "    os.makedirs(PROCESSED_PATH)\n",
    "\n",
    "bigDF.to_csv(PROCESSED_PATH + 'dataset.csv', index=False, columns=['Label', 'Text'])\n",
    "\n",
    "print(bigDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Test, Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:54.571923Z",
     "start_time": "2020-05-17T20:59:54.478421Z"
    }
   },
   "outputs": [],
   "source": [
    "input_file = PROCESSED_PATH + 'dataset.csv';\n",
    "datasetFrame = pd.read_csv(input_file)\n",
    "print('dataset length:', len(datasetFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:54.592033Z",
     "start_time": "2020-05-17T20:59:54.573585Z"
    }
   },
   "outputs": [],
   "source": [
    "non_test_set, test_set = train_test_split(datasetFrame, test_size=0.2, random_state=18030010)\n",
    "train_set, val_set = train_test_split(non_test_set, test_size=0.15, random_state=18030010)\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:54.915264Z",
     "start_time": "2020-05-17T20:59:54.593877Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set.to_csv(PROCESSED_PATH + 'train.csv', index=False, columns=['Label', 'Text'])\n",
    "print('Train Set\\n',train_set)\n",
    "\n",
    "val_set.to_csv(PROCESSED_PATH + 'val.csv', index=False, columns=['Label', 'Text'])\n",
    "print('Val Set\\n',val_set)\n",
    "\n",
    "test_set.to_csv(PROCESSED_PATH + 'test.csv', index=False, columns=['Label', 'Text'])\n",
    "print('Test Set\\n',test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:55.136422Z",
     "start_time": "2020-05-17T20:59:54.917251Z"
    }
   },
   "outputs": [],
   "source": [
    "datasetFrame.Label.value_counts().sort_values(ascending=False).plot(kind='bar', y='# of tweets', title='# of tweets per politician') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:55.255151Z",
     "start_time": "2020-05-17T20:59:55.139670Z"
    }
   },
   "outputs": [],
   "source": [
    "FASTTEXT_PATH = DATASET_PREFIX + '8_fasttext/'\n",
    "\n",
    "if not os.path.exists(FASTTEXT_PATH):\n",
    "    os.makedirs(FASTTEXT_PATH)\n",
    "\n",
    "TRAIN_FILE = PROCESSED_PATH + 'train.csv'\n",
    "VALDN_FILE = PROCESSED_PATH + 'val.csv'\n",
    "TEST_FILE  = PROCESSED_PATH + 'test.csv'\n",
    "\n",
    "trainFrame = pd.read_csv(TRAIN_FILE)\n",
    "valdnFrame = pd.read_csv(VALDN_FILE)\n",
    "testFrame  = pd.read_csv(TEST_FILE)\n",
    "\n",
    "print('dataset length:', len(trainFrame), len(valdnFrame), len(testFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:55.263922Z",
     "start_time": "2020-05-17T20:59:55.257820Z"
    }
   },
   "outputs": [],
   "source": [
    "def modify_for_fasttext(dF):\n",
    "    # add __label__ prefix with labels\n",
    "    dF.Label = '__label__' + dF.Label\n",
    "    return dF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:55.283086Z",
     "start_time": "2020-05-17T20:59:55.266692Z"
    }
   },
   "outputs": [],
   "source": [
    "trnFrame = modify_for_fasttext(trainFrame.copy())\n",
    "valFrame = modify_for_fasttext(valdnFrame.copy())\n",
    "tstFrame = modify_for_fasttext(testFrame.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:59:55.614765Z",
     "start_time": "2020-05-17T20:59:55.284832Z"
    }
   },
   "outputs": [],
   "source": [
    "trnFrame.to_csv(FASTTEXT_PATH + 'train.csv', index=False, sep=' ', \n",
    "                header=False, columns=['Label','Text'])\n",
    "valFrame.to_csv(FASTTEXT_PATH + 'val.csv', index=False, sep=' ', \n",
    "                header=False, columns=['Label','Text'])\n",
    "tstFrame.to_csv(FASTTEXT_PATH + 'test.csv', index=False, sep=' ', \n",
    "                header=False, columns=['Label','Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
